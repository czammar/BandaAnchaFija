{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresion Multinomial y Redes Neuronales para clasificación de municipios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos de penetracion BAF y municipios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HOGARES</th>\n",
       "      <th>POBLACION</th>\n",
       "      <th>ANALF</th>\n",
       "      <th>SPRIM</th>\n",
       "      <th>OVPT</th>\n",
       "      <th>PL5000</th>\n",
       "      <th>PO2SM</th>\n",
       "      <th>DISP_INTERNET</th>\n",
       "      <th>DISP_TV_PAGA</th>\n",
       "      <th>DISP_TEL_CELULAR</th>\n",
       "      <th>DISP_TEL_FIJO</th>\n",
       "      <th>NUM_OPS</th>\n",
       "      <th>DENS_HOGS</th>\n",
       "      <th>ANOS_PROMEDIO_DE_ESCOLARIDAD</th>\n",
       "      <th>INGRESOPC_ANUAL</th>\n",
       "      <th>PEN_CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2446.000000</td>\n",
       "      <td>2.446000e+03</td>\n",
       "      <td>2446.000000</td>\n",
       "      <td>2446.000000</td>\n",
       "      <td>2446.000000</td>\n",
       "      <td>2446.000000</td>\n",
       "      <td>2446.000000</td>\n",
       "      <td>2446.000000</td>\n",
       "      <td>2446.000000</td>\n",
       "      <td>2446.000000</td>\n",
       "      <td>2446.000000</td>\n",
       "      <td>2446.000000</td>\n",
       "      <td>2446.000000</td>\n",
       "      <td>2446.000000</td>\n",
       "      <td>2446.000000</td>\n",
       "      <td>2446.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12991.372036</td>\n",
       "      <td>4.860599e+04</td>\n",
       "      <td>11.690286</td>\n",
       "      <td>29.213050</td>\n",
       "      <td>8.289027</td>\n",
       "      <td>71.898684</td>\n",
       "      <td>55.381063</td>\n",
       "      <td>12.054015</td>\n",
       "      <td>31.158202</td>\n",
       "      <td>57.439859</td>\n",
       "      <td>20.204734</td>\n",
       "      <td>0.226492</td>\n",
       "      <td>7916.018110</td>\n",
       "      <td>6.423426</td>\n",
       "      <td>1935.145724</td>\n",
       "      <td>0.487326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>38143.144359</td>\n",
       "      <td>1.389142e+05</td>\n",
       "      <td>8.527067</td>\n",
       "      <td>11.862555</td>\n",
       "      <td>8.879405</td>\n",
       "      <td>34.685724</td>\n",
       "      <td>16.985484</td>\n",
       "      <td>12.770534</td>\n",
       "      <td>18.829657</td>\n",
       "      <td>25.072398</td>\n",
       "      <td>14.044259</td>\n",
       "      <td>0.418647</td>\n",
       "      <td>35001.243252</td>\n",
       "      <td>1.767869</td>\n",
       "      <td>1020.473455</td>\n",
       "      <td>0.861612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>8.700000e+01</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>2.490000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.995331</td>\n",
       "      <td>1.460000</td>\n",
       "      <td>185.290000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1111.750000</td>\n",
       "      <td>4.253000e+03</td>\n",
       "      <td>5.240000</td>\n",
       "      <td>20.500000</td>\n",
       "      <td>1.882500</td>\n",
       "      <td>42.687500</td>\n",
       "      <td>42.930000</td>\n",
       "      <td>2.100553</td>\n",
       "      <td>15.313693</td>\n",
       "      <td>42.198730</td>\n",
       "      <td>9.047367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>521.867640</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>1198.730000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3471.500000</td>\n",
       "      <td>1.340400e+04</td>\n",
       "      <td>9.725000</td>\n",
       "      <td>29.405000</td>\n",
       "      <td>5.215000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>57.020000</td>\n",
       "      <td>7.954121</td>\n",
       "      <td>29.018092</td>\n",
       "      <td>64.877177</td>\n",
       "      <td>17.645177</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1387.721595</td>\n",
       "      <td>6.280000</td>\n",
       "      <td>1789.905000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8877.500000</td>\n",
       "      <td>3.439950e+04</td>\n",
       "      <td>15.705000</td>\n",
       "      <td>37.345000</td>\n",
       "      <td>11.377500</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>68.470000</td>\n",
       "      <td>17.788759</td>\n",
       "      <td>45.173174</td>\n",
       "      <td>76.445478</td>\n",
       "      <td>29.454433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3462.189955</td>\n",
       "      <td>7.520000</td>\n",
       "      <td>2447.497500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>495665.000000</td>\n",
       "      <td>1.827868e+06</td>\n",
       "      <td>56.420000</td>\n",
       "      <td>71.240000</td>\n",
       "      <td>68.490000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>94.120000</td>\n",
       "      <td>81.882586</td>\n",
       "      <td>85.097192</td>\n",
       "      <td>95.002027</td>\n",
       "      <td>84.047612</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>598127.340824</td>\n",
       "      <td>13.830000</td>\n",
       "      <td>9748.530000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             HOGARES     POBLACION        ANALF        SPRIM         OVPT  \\\n",
       "count    2446.000000  2.446000e+03  2446.000000  2446.000000  2446.000000   \n",
       "mean    12991.372036  4.860599e+04    11.690286    29.213050     8.289027   \n",
       "std     38143.144359  1.389142e+05     8.527067    11.862555     8.879405   \n",
       "min        31.000000  8.700000e+01     0.670000     2.490000     0.000000   \n",
       "25%      1111.750000  4.253000e+03     5.240000    20.500000     1.882500   \n",
       "50%      3471.500000  1.340400e+04     9.725000    29.405000     5.215000   \n",
       "75%      8877.500000  3.439950e+04    15.705000    37.345000    11.377500   \n",
       "max    495665.000000  1.827868e+06    56.420000    71.240000    68.490000   \n",
       "\n",
       "            PL5000        PO2SM  DISP_INTERNET  DISP_TV_PAGA  \\\n",
       "count  2446.000000  2446.000000    2446.000000   2446.000000   \n",
       "mean     71.898684    55.381063      12.054015     31.158202   \n",
       "std      34.685724    16.985484      12.770534     18.829657   \n",
       "min       0.000000     8.250000       0.000000      0.000000   \n",
       "25%      42.687500    42.930000       2.100553     15.313693   \n",
       "50%     100.000000    57.020000       7.954121     29.018092   \n",
       "75%     100.000000    68.470000      17.788759     45.173174   \n",
       "max     100.000000    94.120000      81.882586     85.097192   \n",
       "\n",
       "       DISP_TEL_CELULAR  DISP_TEL_FIJO      NUM_OPS      DENS_HOGS  \\\n",
       "count       2446.000000    2446.000000  2446.000000    2446.000000   \n",
       "mean          57.439859      20.204734     0.226492    7916.018110   \n",
       "std           25.072398      14.044259     0.418647   35001.243252   \n",
       "min            0.000000       0.000000     0.000000       4.995331   \n",
       "25%           42.198730       9.047367     0.000000     521.867640   \n",
       "50%           64.877177      17.645177     0.000000    1387.721595   \n",
       "75%           76.445478      29.454433     0.000000    3462.189955   \n",
       "max           95.002027      84.047612     1.000000  598127.340824   \n",
       "\n",
       "       ANOS_PROMEDIO_DE_ESCOLARIDAD  INGRESOPC_ANUAL    PEN_CLASS  \n",
       "count                   2446.000000      2446.000000  2446.000000  \n",
       "mean                       6.423426      1935.145724     0.487326  \n",
       "std                        1.767869      1020.473455     0.861612  \n",
       "min                        1.460000       185.290000     0.000000  \n",
       "25%                        5.190000      1198.730000     0.000000  \n",
       "50%                        6.280000      1789.905000     0.000000  \n",
       "75%                        7.520000      2447.497500     1.000000  \n",
       "max                       13.830000      9748.530000     5.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv ('BAF_06209_selected.csv')\n",
    "\n",
    "# Elimina columas\n",
    "del data['SATELITAL']\n",
    "del data['TERRESTRE_FIJO_INALAMBRICO']\n",
    "del data['OTRAS_TECNOLOGIAS']\n",
    "del data['SIN_TECNOLOGIA_ESPECIFICADA']\n",
    "del data['CABLE_COAXIAL']\n",
    "del data['DSL']\n",
    "del data['FIBRA_OPTICA']\n",
    "del data['COAX_FO']\n",
    "del data['ALL_ACCESS']\n",
    "del data['PEN_BAF']\n",
    "del data['PEN_BAF_COAXFO']\n",
    "\n",
    "\n",
    "# Eliminia indices de base Naciones Unidas y CONAPO\n",
    "del data['INDICE_DE_EDUCACION']\n",
    "del data['INDICE_DE_SALUD']\n",
    "del data['INDICE_DE_INGRESO']\n",
    "del data['IDH']\n",
    "del data['IM']\n",
    "del data['TASA_DE_MORTALIDAD_INFANTIL']\n",
    "del data['OVSDE']\n",
    "del data['OVSEE']\n",
    "del data['OVSAE']\n",
    "del data['VHAC']\n",
    "\n",
    "\n",
    "del data['ANOS_ESPERADOS_DE_ESCOLARIZACIÓN']\n",
    "\n",
    "# Renombramos una columna para que no de problemas\n",
    "data.rename(columns={\"PL<5000\": \"PL5000\"}, inplace=True)\n",
    "\n",
    "#del data['HOGARES'] # hogares de los municipios\n",
    "#del data['POBLACION'] # hogares de los municipios\n",
    "del data['SUPERFICIE'] # superficie de municipios\n",
    "#del data['ALL_ACCESS'] # numero total de accesos BAF\n",
    "\n",
    "# Sustitumos valores de la columnas NUM_OPS (1= Hay mas de dos operadores, 0 = en otro caso)\n",
    "data['NUM_OPS'] = np.where(data['NUM_OPS']>1,1,0)\n",
    "\n",
    "#data[\"PEN_CLASS\"] = data[\"PEN_CLASS\"].astype('category')\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sz = data.shape\n",
    "m,n = sz\n",
    "\n",
    "# Definimos conjuntos de entrenamiento y de prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, :n-1], data.iloc[:, n-1], random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HOGARES</th>\n",
       "      <th>POBLACION</th>\n",
       "      <th>ANALF</th>\n",
       "      <th>SPRIM</th>\n",
       "      <th>OVPT</th>\n",
       "      <th>PL5000</th>\n",
       "      <th>PO2SM</th>\n",
       "      <th>DISP_INTERNET</th>\n",
       "      <th>DISP_TV_PAGA</th>\n",
       "      <th>DISP_TEL_CELULAR</th>\n",
       "      <th>DISP_TEL_FIJO</th>\n",
       "      <th>NUM_OPS</th>\n",
       "      <th>DENS_HOGS</th>\n",
       "      <th>ANOS_PROMEDIO_DE_ESCOLARIDAD</th>\n",
       "      <th>INGRESOPC_ANUAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230559</td>\n",
       "      <td>877190</td>\n",
       "      <td>2.06</td>\n",
       "      <td>9.54</td>\n",
       "      <td>0.63</td>\n",
       "      <td>8.73</td>\n",
       "      <td>31.13</td>\n",
       "      <td>42.075713</td>\n",
       "      <td>51.496795</td>\n",
       "      <td>88.942299</td>\n",
       "      <td>44.052334</td>\n",
       "      <td>1</td>\n",
       "      <td>19569.745531</td>\n",
       "      <td>9.96</td>\n",
       "      <td>4015.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10787</td>\n",
       "      <td>46464</td>\n",
       "      <td>4.47</td>\n",
       "      <td>20.89</td>\n",
       "      <td>1.69</td>\n",
       "      <td>100.00</td>\n",
       "      <td>52.76</td>\n",
       "      <td>9.826643</td>\n",
       "      <td>38.045796</td>\n",
       "      <td>71.474924</td>\n",
       "      <td>16.288125</td>\n",
       "      <td>1</td>\n",
       "      <td>1969.400982</td>\n",
       "      <td>7.04</td>\n",
       "      <td>1940.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14182</td>\n",
       "      <td>56048</td>\n",
       "      <td>4.80</td>\n",
       "      <td>24.18</td>\n",
       "      <td>0.93</td>\n",
       "      <td>50.76</td>\n",
       "      <td>61.95</td>\n",
       "      <td>17.534943</td>\n",
       "      <td>34.236905</td>\n",
       "      <td>71.798673</td>\n",
       "      <td>37.978258</td>\n",
       "      <td>1</td>\n",
       "      <td>1520.466582</td>\n",
       "      <td>6.71</td>\n",
       "      <td>2054.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3519</td>\n",
       "      <td>15577</td>\n",
       "      <td>4.35</td>\n",
       "      <td>16.55</td>\n",
       "      <td>1.42</td>\n",
       "      <td>100.00</td>\n",
       "      <td>49.79</td>\n",
       "      <td>9.065075</td>\n",
       "      <td>38.846263</td>\n",
       "      <td>74.140381</td>\n",
       "      <td>15.913612</td>\n",
       "      <td>0</td>\n",
       "      <td>2712.347772</td>\n",
       "      <td>7.89</td>\n",
       "      <td>2303.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4882</td>\n",
       "      <td>20245</td>\n",
       "      <td>4.92</td>\n",
       "      <td>21.05</td>\n",
       "      <td>1.65</td>\n",
       "      <td>71.33</td>\n",
       "      <td>48.80</td>\n",
       "      <td>5.598852</td>\n",
       "      <td>26.558655</td>\n",
       "      <td>71.472518</td>\n",
       "      <td>10.664479</td>\n",
       "      <td>0</td>\n",
       "      <td>956.898410</td>\n",
       "      <td>7.05</td>\n",
       "      <td>1881.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28911</td>\n",
       "      <td>120405</td>\n",
       "      <td>3.26</td>\n",
       "      <td>13.73</td>\n",
       "      <td>1.03</td>\n",
       "      <td>45.17</td>\n",
       "      <td>33.77</td>\n",
       "      <td>34.223377</td>\n",
       "      <td>46.161039</td>\n",
       "      <td>88.651082</td>\n",
       "      <td>38.552381</td>\n",
       "      <td>1</td>\n",
       "      <td>5723.363820</td>\n",
       "      <td>9.18</td>\n",
       "      <td>3949.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10769</td>\n",
       "      <td>46473</td>\n",
       "      <td>3.41</td>\n",
       "      <td>14.90</td>\n",
       "      <td>0.62</td>\n",
       "      <td>31.60</td>\n",
       "      <td>41.48</td>\n",
       "      <td>25.359829</td>\n",
       "      <td>43.179497</td>\n",
       "      <td>81.688179</td>\n",
       "      <td>27.569876</td>\n",
       "      <td>0</td>\n",
       "      <td>5440.812408</td>\n",
       "      <td>8.65</td>\n",
       "      <td>3102.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12223</td>\n",
       "      <td>53866</td>\n",
       "      <td>3.53</td>\n",
       "      <td>14.75</td>\n",
       "      <td>0.91</td>\n",
       "      <td>43.06</td>\n",
       "      <td>43.44</td>\n",
       "      <td>18.441516</td>\n",
       "      <td>35.049521</td>\n",
       "      <td>76.393550</td>\n",
       "      <td>21.838422</td>\n",
       "      <td>1</td>\n",
       "      <td>3248.983281</td>\n",
       "      <td>8.37</td>\n",
       "      <td>2471.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11705</td>\n",
       "      <td>46454</td>\n",
       "      <td>2.94</td>\n",
       "      <td>13.77</td>\n",
       "      <td>0.46</td>\n",
       "      <td>54.92</td>\n",
       "      <td>35.49</td>\n",
       "      <td>21.680055</td>\n",
       "      <td>48.564348</td>\n",
       "      <td>90.146983</td>\n",
       "      <td>16.296360</td>\n",
       "      <td>1</td>\n",
       "      <td>8415.414480</td>\n",
       "      <td>8.38</td>\n",
       "      <td>2717.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2104</td>\n",
       "      <td>8896</td>\n",
       "      <td>3.20</td>\n",
       "      <td>14.87</td>\n",
       "      <td>0.44</td>\n",
       "      <td>100.00</td>\n",
       "      <td>44.93</td>\n",
       "      <td>10.376011</td>\n",
       "      <td>43.503094</td>\n",
       "      <td>78.962399</td>\n",
       "      <td>23.845788</td>\n",
       "      <td>0</td>\n",
       "      <td>242.762695</td>\n",
       "      <td>7.95</td>\n",
       "      <td>2272.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4948</td>\n",
       "      <td>20926</td>\n",
       "      <td>4.23</td>\n",
       "      <td>22.13</td>\n",
       "      <td>1.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>49.39</td>\n",
       "      <td>9.236055</td>\n",
       "      <td>32.417138</td>\n",
       "      <td>72.797090</td>\n",
       "      <td>14.207761</td>\n",
       "      <td>0</td>\n",
       "      <td>2132.115310</td>\n",
       "      <td>7.24</td>\n",
       "      <td>2176.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>143169</td>\n",
       "      <td>486639</td>\n",
       "      <td>3.58</td>\n",
       "      <td>14.67</td>\n",
       "      <td>1.28</td>\n",
       "      <td>22.82</td>\n",
       "      <td>29.63</td>\n",
       "      <td>44.793172</td>\n",
       "      <td>66.046890</td>\n",
       "      <td>90.681400</td>\n",
       "      <td>38.056382</td>\n",
       "      <td>1</td>\n",
       "      <td>269.424652</td>\n",
       "      <td>9.21</td>\n",
       "      <td>3554.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>291763</td>\n",
       "      <td>988417</td>\n",
       "      <td>1.55</td>\n",
       "      <td>9.36</td>\n",
       "      <td>0.69</td>\n",
       "      <td>12.86</td>\n",
       "      <td>23.09</td>\n",
       "      <td>48.970065</td>\n",
       "      <td>51.078857</td>\n",
       "      <td>91.682314</td>\n",
       "      <td>44.351463</td>\n",
       "      <td>1</td>\n",
       "      <td>1863.894193</td>\n",
       "      <td>9.83</td>\n",
       "      <td>4124.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>27823</td>\n",
       "      <td>96734</td>\n",
       "      <td>2.31</td>\n",
       "      <td>11.56</td>\n",
       "      <td>1.48</td>\n",
       "      <td>21.49</td>\n",
       "      <td>25.72</td>\n",
       "      <td>46.079203</td>\n",
       "      <td>67.314825</td>\n",
       "      <td>89.086209</td>\n",
       "      <td>33.573115</td>\n",
       "      <td>1</td>\n",
       "      <td>5560.818643</td>\n",
       "      <td>8.81</td>\n",
       "      <td>3365.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>29767</td>\n",
       "      <td>102406</td>\n",
       "      <td>2.32</td>\n",
       "      <td>12.77</td>\n",
       "      <td>1.17</td>\n",
       "      <td>29.39</td>\n",
       "      <td>25.36</td>\n",
       "      <td>46.167325</td>\n",
       "      <td>63.332761</td>\n",
       "      <td>90.458684</td>\n",
       "      <td>33.829239</td>\n",
       "      <td>1</td>\n",
       "      <td>1107.720590</td>\n",
       "      <td>8.92</td>\n",
       "      <td>3898.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>475341</td>\n",
       "      <td>1641570</td>\n",
       "      <td>1.69</td>\n",
       "      <td>9.68</td>\n",
       "      <td>1.37</td>\n",
       "      <td>3.23</td>\n",
       "      <td>20.47</td>\n",
       "      <td>51.287538</td>\n",
       "      <td>57.390643</td>\n",
       "      <td>91.136289</td>\n",
       "      <td>41.580929</td>\n",
       "      <td>1</td>\n",
       "      <td>38497.254483</td>\n",
       "      <td>9.47</td>\n",
       "      <td>3936.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>21436</td>\n",
       "      <td>72564</td>\n",
       "      <td>5.06</td>\n",
       "      <td>20.08</td>\n",
       "      <td>3.12</td>\n",
       "      <td>22.03</td>\n",
       "      <td>40.10</td>\n",
       "      <td>27.272727</td>\n",
       "      <td>57.697344</td>\n",
       "      <td>86.901422</td>\n",
       "      <td>28.951553</td>\n",
       "      <td>1</td>\n",
       "      <td>116.935394</td>\n",
       "      <td>8.08</td>\n",
       "      <td>3347.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>83027</td>\n",
       "      <td>272711</td>\n",
       "      <td>1.86</td>\n",
       "      <td>8.43</td>\n",
       "      <td>1.69</td>\n",
       "      <td>12.52</td>\n",
       "      <td>22.24</td>\n",
       "      <td>50.719377</td>\n",
       "      <td>63.795994</td>\n",
       "      <td>93.892835</td>\n",
       "      <td>46.692555</td>\n",
       "      <td>1</td>\n",
       "      <td>524.387787</td>\n",
       "      <td>10.58</td>\n",
       "      <td>4756.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5975</td>\n",
       "      <td>18912</td>\n",
       "      <td>2.49</td>\n",
       "      <td>11.38</td>\n",
       "      <td>3.65</td>\n",
       "      <td>12.03</td>\n",
       "      <td>27.46</td>\n",
       "      <td>42.318841</td>\n",
       "      <td>61.892583</td>\n",
       "      <td>86.717818</td>\n",
       "      <td>35.754476</td>\n",
       "      <td>0</td>\n",
       "      <td>129.107677</td>\n",
       "      <td>9.84</td>\n",
       "      <td>3251.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>81089</td>\n",
       "      <td>287671</td>\n",
       "      <td>1.96</td>\n",
       "      <td>10.66</td>\n",
       "      <td>7.03</td>\n",
       "      <td>9.62</td>\n",
       "      <td>17.32</td>\n",
       "      <td>43.367860</td>\n",
       "      <td>59.934255</td>\n",
       "      <td>94.533275</td>\n",
       "      <td>31.624388</td>\n",
       "      <td>1</td>\n",
       "      <td>2161.768038</td>\n",
       "      <td>9.56</td>\n",
       "      <td>4450.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>18307</td>\n",
       "      <td>60171</td>\n",
       "      <td>5.06</td>\n",
       "      <td>20.77</td>\n",
       "      <td>2.22</td>\n",
       "      <td>46.34</td>\n",
       "      <td>28.16</td>\n",
       "      <td>30.413986</td>\n",
       "      <td>70.102809</td>\n",
       "      <td>87.497938</td>\n",
       "      <td>32.871516</td>\n",
       "      <td>0</td>\n",
       "      <td>57.105621</td>\n",
       "      <td>7.91</td>\n",
       "      <td>3633.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7000</td>\n",
       "      <td>28424</td>\n",
       "      <td>15.29</td>\n",
       "      <td>29.49</td>\n",
       "      <td>11.15</td>\n",
       "      <td>100.00</td>\n",
       "      <td>44.78</td>\n",
       "      <td>1.528571</td>\n",
       "      <td>48.685714</td>\n",
       "      <td>41.700000</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>0</td>\n",
       "      <td>50.044790</td>\n",
       "      <td>5.59</td>\n",
       "      <td>1575.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>14209</td>\n",
       "      <td>56537</td>\n",
       "      <td>11.10</td>\n",
       "      <td>23.25</td>\n",
       "      <td>2.54</td>\n",
       "      <td>26.28</td>\n",
       "      <td>54.47</td>\n",
       "      <td>15.727964</td>\n",
       "      <td>73.979161</td>\n",
       "      <td>68.635596</td>\n",
       "      <td>11.531963</td>\n",
       "      <td>0</td>\n",
       "      <td>675.750226</td>\n",
       "      <td>8.06</td>\n",
       "      <td>2595.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>79159</td>\n",
       "      <td>283025</td>\n",
       "      <td>3.98</td>\n",
       "      <td>11.98</td>\n",
       "      <td>0.97</td>\n",
       "      <td>9.71</td>\n",
       "      <td>38.21</td>\n",
       "      <td>41.895426</td>\n",
       "      <td>61.216034</td>\n",
       "      <td>89.157266</td>\n",
       "      <td>31.425359</td>\n",
       "      <td>1</td>\n",
       "      <td>2440.173983</td>\n",
       "      <td>9.95</td>\n",
       "      <td>3951.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>11592</td>\n",
       "      <td>43879</td>\n",
       "      <td>12.80</td>\n",
       "      <td>30.29</td>\n",
       "      <td>13.47</td>\n",
       "      <td>76.18</td>\n",
       "      <td>60.34</td>\n",
       "      <td>3.651783</td>\n",
       "      <td>50.086535</td>\n",
       "      <td>39.728280</td>\n",
       "      <td>10.756317</td>\n",
       "      <td>0</td>\n",
       "      <td>204.461032</td>\n",
       "      <td>6.02</td>\n",
       "      <td>1778.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>70615</td>\n",
       "      <td>248303</td>\n",
       "      <td>4.08</td>\n",
       "      <td>14.07</td>\n",
       "      <td>2.12</td>\n",
       "      <td>17.25</td>\n",
       "      <td>22.42</td>\n",
       "      <td>41.372758</td>\n",
       "      <td>69.532724</td>\n",
       "      <td>90.396370</td>\n",
       "      <td>32.264057</td>\n",
       "      <td>1</td>\n",
       "      <td>819.464769</td>\n",
       "      <td>9.73</td>\n",
       "      <td>4743.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>24189</td>\n",
       "      <td>90244</td>\n",
       "      <td>10.04</td>\n",
       "      <td>26.32</td>\n",
       "      <td>3.65</td>\n",
       "      <td>52.31</td>\n",
       "      <td>56.78</td>\n",
       "      <td>12.924973</td>\n",
       "      <td>69.137232</td>\n",
       "      <td>74.360989</td>\n",
       "      <td>12.188767</td>\n",
       "      <td>1</td>\n",
       "      <td>352.842844</td>\n",
       "      <td>7.06</td>\n",
       "      <td>2354.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>15534</td>\n",
       "      <td>58553</td>\n",
       "      <td>9.55</td>\n",
       "      <td>26.37</td>\n",
       "      <td>5.12</td>\n",
       "      <td>45.60</td>\n",
       "      <td>58.84</td>\n",
       "      <td>10.360825</td>\n",
       "      <td>41.411082</td>\n",
       "      <td>74.117268</td>\n",
       "      <td>14.536082</td>\n",
       "      <td>0</td>\n",
       "      <td>324.755555</td>\n",
       "      <td>6.81</td>\n",
       "      <td>2344.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7651</td>\n",
       "      <td>31230</td>\n",
       "      <td>9.67</td>\n",
       "      <td>25.12</td>\n",
       "      <td>1.02</td>\n",
       "      <td>32.95</td>\n",
       "      <td>51.36</td>\n",
       "      <td>7.672200</td>\n",
       "      <td>70.186904</td>\n",
       "      <td>65.952163</td>\n",
       "      <td>6.090707</td>\n",
       "      <td>1</td>\n",
       "      <td>600.092551</td>\n",
       "      <td>8.01</td>\n",
       "      <td>2472.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9307</td>\n",
       "      <td>40100</td>\n",
       "      <td>10.12</td>\n",
       "      <td>35.91</td>\n",
       "      <td>2.45</td>\n",
       "      <td>80.69</td>\n",
       "      <td>66.58</td>\n",
       "      <td>5.848833</td>\n",
       "      <td>59.918288</td>\n",
       "      <td>49.199011</td>\n",
       "      <td>9.332330</td>\n",
       "      <td>0</td>\n",
       "      <td>119.629095</td>\n",
       "      <td>6.06</td>\n",
       "      <td>1815.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2447</th>\n",
       "      <td>5818</td>\n",
       "      <td>23827</td>\n",
       "      <td>3.48</td>\n",
       "      <td>18.42</td>\n",
       "      <td>1.37</td>\n",
       "      <td>39.22</td>\n",
       "      <td>54.56</td>\n",
       "      <td>13.819182</td>\n",
       "      <td>31.024407</td>\n",
       "      <td>75.782056</td>\n",
       "      <td>32.640083</td>\n",
       "      <td>1</td>\n",
       "      <td>526.120651</td>\n",
       "      <td>7.13</td>\n",
       "      <td>1995.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448</th>\n",
       "      <td>728</td>\n",
       "      <td>2404</td>\n",
       "      <td>5.77</td>\n",
       "      <td>30.22</td>\n",
       "      <td>0.50</td>\n",
       "      <td>100.00</td>\n",
       "      <td>54.43</td>\n",
       "      <td>17.744154</td>\n",
       "      <td>31.086657</td>\n",
       "      <td>74.277854</td>\n",
       "      <td>37.826685</td>\n",
       "      <td>0</td>\n",
       "      <td>449.910389</td>\n",
       "      <td>6.34</td>\n",
       "      <td>1733.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2449</th>\n",
       "      <td>2815</td>\n",
       "      <td>9129</td>\n",
       "      <td>6.07</td>\n",
       "      <td>30.11</td>\n",
       "      <td>1.09</td>\n",
       "      <td>100.00</td>\n",
       "      <td>50.41</td>\n",
       "      <td>14.290793</td>\n",
       "      <td>44.756488</td>\n",
       "      <td>65.908283</td>\n",
       "      <td>40.028439</td>\n",
       "      <td>0</td>\n",
       "      <td>175.057834</td>\n",
       "      <td>6.44</td>\n",
       "      <td>1862.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2450</th>\n",
       "      <td>3306</td>\n",
       "      <td>12354</td>\n",
       "      <td>2.62</td>\n",
       "      <td>16.52</td>\n",
       "      <td>0.36</td>\n",
       "      <td>42.55</td>\n",
       "      <td>34.78</td>\n",
       "      <td>23.855714</td>\n",
       "      <td>42.194604</td>\n",
       "      <td>75.113671</td>\n",
       "      <td>40.375871</td>\n",
       "      <td>1</td>\n",
       "      <td>1827.326995</td>\n",
       "      <td>8.30</td>\n",
       "      <td>2638.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2451</th>\n",
       "      <td>1220</td>\n",
       "      <td>3947</td>\n",
       "      <td>6.72</td>\n",
       "      <td>34.68</td>\n",
       "      <td>0.63</td>\n",
       "      <td>100.00</td>\n",
       "      <td>41.69</td>\n",
       "      <td>13.781788</td>\n",
       "      <td>32.403610</td>\n",
       "      <td>61.033634</td>\n",
       "      <td>40.196883</td>\n",
       "      <td>0</td>\n",
       "      <td>225.821379</td>\n",
       "      <td>5.94</td>\n",
       "      <td>1688.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2452</th>\n",
       "      <td>8022</td>\n",
       "      <td>27750</td>\n",
       "      <td>6.21</td>\n",
       "      <td>26.73</td>\n",
       "      <td>0.78</td>\n",
       "      <td>40.71</td>\n",
       "      <td>46.81</td>\n",
       "      <td>19.808028</td>\n",
       "      <td>30.865121</td>\n",
       "      <td>73.186238</td>\n",
       "      <td>35.078534</td>\n",
       "      <td>0</td>\n",
       "      <td>914.031789</td>\n",
       "      <td>6.78</td>\n",
       "      <td>2036.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2453</th>\n",
       "      <td>4078</td>\n",
       "      <td>16864</td>\n",
       "      <td>6.58</td>\n",
       "      <td>24.67</td>\n",
       "      <td>2.01</td>\n",
       "      <td>100.00</td>\n",
       "      <td>54.87</td>\n",
       "      <td>5.909760</td>\n",
       "      <td>36.145169</td>\n",
       "      <td>67.312408</td>\n",
       "      <td>18.391368</td>\n",
       "      <td>0</td>\n",
       "      <td>998.335292</td>\n",
       "      <td>6.52</td>\n",
       "      <td>1376.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2454</th>\n",
       "      <td>10315</td>\n",
       "      <td>43471</td>\n",
       "      <td>6.12</td>\n",
       "      <td>20.43</td>\n",
       "      <td>3.58</td>\n",
       "      <td>48.82</td>\n",
       "      <td>60.71</td>\n",
       "      <td>11.065852</td>\n",
       "      <td>31.917370</td>\n",
       "      <td>67.723790</td>\n",
       "      <td>25.545534</td>\n",
       "      <td>1</td>\n",
       "      <td>1599.696034</td>\n",
       "      <td>7.28</td>\n",
       "      <td>1890.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2455</th>\n",
       "      <td>4636</td>\n",
       "      <td>17647</td>\n",
       "      <td>6.10</td>\n",
       "      <td>26.87</td>\n",
       "      <td>1.01</td>\n",
       "      <td>69.06</td>\n",
       "      <td>50.18</td>\n",
       "      <td>7.444972</td>\n",
       "      <td>17.350022</td>\n",
       "      <td>66.465257</td>\n",
       "      <td>15.753129</td>\n",
       "      <td>0</td>\n",
       "      <td>790.950813</td>\n",
       "      <td>6.37</td>\n",
       "      <td>1875.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2456</th>\n",
       "      <td>18940</td>\n",
       "      <td>73151</td>\n",
       "      <td>8.85</td>\n",
       "      <td>24.28</td>\n",
       "      <td>0.95</td>\n",
       "      <td>91.97</td>\n",
       "      <td>60.85</td>\n",
       "      <td>3.495248</td>\n",
       "      <td>25.781415</td>\n",
       "      <td>50.786695</td>\n",
       "      <td>17.861668</td>\n",
       "      <td>0</td>\n",
       "      <td>597.695056</td>\n",
       "      <td>6.63</td>\n",
       "      <td>1322.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2457</th>\n",
       "      <td>17151</td>\n",
       "      <td>63880</td>\n",
       "      <td>4.44</td>\n",
       "      <td>22.19</td>\n",
       "      <td>0.84</td>\n",
       "      <td>47.45</td>\n",
       "      <td>50.80</td>\n",
       "      <td>20.315053</td>\n",
       "      <td>38.920653</td>\n",
       "      <td>70.490082</td>\n",
       "      <td>40.752625</td>\n",
       "      <td>0</td>\n",
       "      <td>932.778594</td>\n",
       "      <td>7.71</td>\n",
       "      <td>2383.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2458</th>\n",
       "      <td>5499</td>\n",
       "      <td>22278</td>\n",
       "      <td>5.14</td>\n",
       "      <td>23.76</td>\n",
       "      <td>1.56</td>\n",
       "      <td>76.61</td>\n",
       "      <td>65.16</td>\n",
       "      <td>6.040757</td>\n",
       "      <td>26.109898</td>\n",
       "      <td>43.067686</td>\n",
       "      <td>36.208151</td>\n",
       "      <td>0</td>\n",
       "      <td>388.627401</td>\n",
       "      <td>6.68</td>\n",
       "      <td>1491.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2459</th>\n",
       "      <td>776</td>\n",
       "      <td>2656</td>\n",
       "      <td>5.47</td>\n",
       "      <td>31.05</td>\n",
       "      <td>4.40</td>\n",
       "      <td>100.00</td>\n",
       "      <td>51.32</td>\n",
       "      <td>15.870968</td>\n",
       "      <td>24.774194</td>\n",
       "      <td>33.935484</td>\n",
       "      <td>36.903226</td>\n",
       "      <td>1</td>\n",
       "      <td>278.785701</td>\n",
       "      <td>6.56</td>\n",
       "      <td>1784.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2460</th>\n",
       "      <td>16865</td>\n",
       "      <td>62433</td>\n",
       "      <td>3.99</td>\n",
       "      <td>21.01</td>\n",
       "      <td>1.76</td>\n",
       "      <td>64.53</td>\n",
       "      <td>46.18</td>\n",
       "      <td>14.930329</td>\n",
       "      <td>32.428106</td>\n",
       "      <td>68.069967</td>\n",
       "      <td>28.787430</td>\n",
       "      <td>1</td>\n",
       "      <td>468.190008</td>\n",
       "      <td>7.41</td>\n",
       "      <td>2159.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461</th>\n",
       "      <td>370</td>\n",
       "      <td>1329</td>\n",
       "      <td>8.07</td>\n",
       "      <td>34.51</td>\n",
       "      <td>1.96</td>\n",
       "      <td>100.00</td>\n",
       "      <td>59.53</td>\n",
       "      <td>7.027027</td>\n",
       "      <td>23.513514</td>\n",
       "      <td>33.243243</td>\n",
       "      <td>56.216216</td>\n",
       "      <td>0</td>\n",
       "      <td>185.370741</td>\n",
       "      <td>5.72</td>\n",
       "      <td>1494.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>4095</td>\n",
       "      <td>15484</td>\n",
       "      <td>6.42</td>\n",
       "      <td>25.03</td>\n",
       "      <td>1.24</td>\n",
       "      <td>53.26</td>\n",
       "      <td>59.53</td>\n",
       "      <td>12.576313</td>\n",
       "      <td>41.318681</td>\n",
       "      <td>66.764347</td>\n",
       "      <td>34.554335</td>\n",
       "      <td>0</td>\n",
       "      <td>998.317853</td>\n",
       "      <td>6.69</td>\n",
       "      <td>2008.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>2678</td>\n",
       "      <td>8855</td>\n",
       "      <td>6.03</td>\n",
       "      <td>29.96</td>\n",
       "      <td>2.51</td>\n",
       "      <td>100.00</td>\n",
       "      <td>53.95</td>\n",
       "      <td>13.757009</td>\n",
       "      <td>36.785047</td>\n",
       "      <td>68.598131</td>\n",
       "      <td>27.327103</td>\n",
       "      <td>0</td>\n",
       "      <td>491.881566</td>\n",
       "      <td>6.79</td>\n",
       "      <td>1955.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>2198</td>\n",
       "      <td>7071</td>\n",
       "      <td>6.96</td>\n",
       "      <td>40.89</td>\n",
       "      <td>1.94</td>\n",
       "      <td>100.00</td>\n",
       "      <td>62.78</td>\n",
       "      <td>10.191083</td>\n",
       "      <td>31.710646</td>\n",
       "      <td>50.318471</td>\n",
       "      <td>44.040036</td>\n",
       "      <td>0</td>\n",
       "      <td>303.369081</td>\n",
       "      <td>5.56</td>\n",
       "      <td>1595.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>1624</td>\n",
       "      <td>5378</td>\n",
       "      <td>4.05</td>\n",
       "      <td>23.72</td>\n",
       "      <td>1.51</td>\n",
       "      <td>100.00</td>\n",
       "      <td>44.32</td>\n",
       "      <td>25.862069</td>\n",
       "      <td>45.258621</td>\n",
       "      <td>65.086207</td>\n",
       "      <td>43.103448</td>\n",
       "      <td>0</td>\n",
       "      <td>239.009816</td>\n",
       "      <td>7.61</td>\n",
       "      <td>2291.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>7484</td>\n",
       "      <td>26645</td>\n",
       "      <td>4.34</td>\n",
       "      <td>20.09</td>\n",
       "      <td>1.11</td>\n",
       "      <td>35.68</td>\n",
       "      <td>47.96</td>\n",
       "      <td>22.893822</td>\n",
       "      <td>38.874030</td>\n",
       "      <td>77.493982</td>\n",
       "      <td>27.641081</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.270953</td>\n",
       "      <td>7.79</td>\n",
       "      <td>2414.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>4435</td>\n",
       "      <td>19413</td>\n",
       "      <td>5.64</td>\n",
       "      <td>18.13</td>\n",
       "      <td>1.41</td>\n",
       "      <td>17.20</td>\n",
       "      <td>46.50</td>\n",
       "      <td>12.051456</td>\n",
       "      <td>32.408034</td>\n",
       "      <td>67.682239</td>\n",
       "      <td>18.663958</td>\n",
       "      <td>0</td>\n",
       "      <td>2008.514107</td>\n",
       "      <td>7.26</td>\n",
       "      <td>2091.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>908</td>\n",
       "      <td>2884</td>\n",
       "      <td>5.99</td>\n",
       "      <td>32.89</td>\n",
       "      <td>1.01</td>\n",
       "      <td>100.00</td>\n",
       "      <td>34.70</td>\n",
       "      <td>20.704846</td>\n",
       "      <td>41.409692</td>\n",
       "      <td>66.299559</td>\n",
       "      <td>47.907489</td>\n",
       "      <td>0</td>\n",
       "      <td>295.659536</td>\n",
       "      <td>6.34</td>\n",
       "      <td>2068.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>9049</td>\n",
       "      <td>32606</td>\n",
       "      <td>5.82</td>\n",
       "      <td>24.82</td>\n",
       "      <td>3.52</td>\n",
       "      <td>61.23</td>\n",
       "      <td>50.75</td>\n",
       "      <td>11.614543</td>\n",
       "      <td>35.794010</td>\n",
       "      <td>54.834788</td>\n",
       "      <td>34.047961</td>\n",
       "      <td>0</td>\n",
       "      <td>158.521887</td>\n",
       "      <td>6.89</td>\n",
       "      <td>1758.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>2379</td>\n",
       "      <td>9796</td>\n",
       "      <td>4.91</td>\n",
       "      <td>20.16</td>\n",
       "      <td>0.72</td>\n",
       "      <td>100.00</td>\n",
       "      <td>42.82</td>\n",
       "      <td>12.232030</td>\n",
       "      <td>24.506095</td>\n",
       "      <td>70.701976</td>\n",
       "      <td>24.127785</td>\n",
       "      <td>0</td>\n",
       "      <td>1487.153841</td>\n",
       "      <td>7.31</td>\n",
       "      <td>1874.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2471</th>\n",
       "      <td>9496</td>\n",
       "      <td>35252</td>\n",
       "      <td>5.56</td>\n",
       "      <td>26.49</td>\n",
       "      <td>2.49</td>\n",
       "      <td>68.64</td>\n",
       "      <td>56.63</td>\n",
       "      <td>7.561078</td>\n",
       "      <td>23.631003</td>\n",
       "      <td>47.335720</td>\n",
       "      <td>26.116259</td>\n",
       "      <td>0</td>\n",
       "      <td>144.350771</td>\n",
       "      <td>6.49</td>\n",
       "      <td>1696.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2472</th>\n",
       "      <td>4632</td>\n",
       "      <td>18745</td>\n",
       "      <td>3.71</td>\n",
       "      <td>17.48</td>\n",
       "      <td>1.39</td>\n",
       "      <td>64.23</td>\n",
       "      <td>50.32</td>\n",
       "      <td>8.725702</td>\n",
       "      <td>39.546436</td>\n",
       "      <td>68.466523</td>\n",
       "      <td>18.444924</td>\n",
       "      <td>0</td>\n",
       "      <td>1355.416398</td>\n",
       "      <td>7.90</td>\n",
       "      <td>2249.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>3165</td>\n",
       "      <td>13099</td>\n",
       "      <td>5.86</td>\n",
       "      <td>25.69</td>\n",
       "      <td>0.97</td>\n",
       "      <td>52.06</td>\n",
       "      <td>51.56</td>\n",
       "      <td>18.526715</td>\n",
       "      <td>29.465697</td>\n",
       "      <td>62.282643</td>\n",
       "      <td>33.797028</td>\n",
       "      <td>0</td>\n",
       "      <td>732.740658</td>\n",
       "      <td>6.99</td>\n",
       "      <td>1821.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>4678</td>\n",
       "      <td>19155</td>\n",
       "      <td>8.23</td>\n",
       "      <td>23.55</td>\n",
       "      <td>2.17</td>\n",
       "      <td>100.00</td>\n",
       "      <td>63.27</td>\n",
       "      <td>7.631466</td>\n",
       "      <td>26.742198</td>\n",
       "      <td>64.920906</td>\n",
       "      <td>18.982471</td>\n",
       "      <td>0</td>\n",
       "      <td>1246.270247</td>\n",
       "      <td>6.73</td>\n",
       "      <td>1519.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>8477</td>\n",
       "      <td>30240</td>\n",
       "      <td>4.83</td>\n",
       "      <td>27.10</td>\n",
       "      <td>0.92</td>\n",
       "      <td>58.26</td>\n",
       "      <td>53.56</td>\n",
       "      <td>12.749380</td>\n",
       "      <td>38.944635</td>\n",
       "      <td>65.033644</td>\n",
       "      <td>37.374572</td>\n",
       "      <td>1</td>\n",
       "      <td>389.003102</td>\n",
       "      <td>6.61</td>\n",
       "      <td>1954.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2476</th>\n",
       "      <td>40236</td>\n",
       "      <td>146147</td>\n",
       "      <td>1.25</td>\n",
       "      <td>8.32</td>\n",
       "      <td>0.22</td>\n",
       "      <td>6.63</td>\n",
       "      <td>28.48</td>\n",
       "      <td>55.180130</td>\n",
       "      <td>59.844360</td>\n",
       "      <td>88.538326</td>\n",
       "      <td>48.976903</td>\n",
       "      <td>1</td>\n",
       "      <td>9113.063961</td>\n",
       "      <td>10.91</td>\n",
       "      <td>4220.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2446 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HOGARES  POBLACION  ANALF  SPRIM   OVPT  PL5000  PO2SM  DISP_INTERNET  \\\n",
       "0      230559     877190   2.06   9.54   0.63    8.73  31.13      42.075713   \n",
       "1       10787      46464   4.47  20.89   1.69  100.00  52.76       9.826643   \n",
       "2       14182      56048   4.80  24.18   0.93   50.76  61.95      17.534943   \n",
       "3        3519      15577   4.35  16.55   1.42  100.00  49.79       9.065075   \n",
       "4        4882      20245   4.92  21.05   1.65   71.33  48.80       5.598852   \n",
       "5       28911     120405   3.26  13.73   1.03   45.17  33.77      34.223377   \n",
       "6       10769      46473   3.41  14.90   0.62   31.60  41.48      25.359829   \n",
       "7       12223      53866   3.53  14.75   0.91   43.06  43.44      18.441516   \n",
       "8       11705      46454   2.94  13.77   0.46   54.92  35.49      21.680055   \n",
       "9        2104       8896   3.20  14.87   0.44  100.00  44.93      10.376011   \n",
       "10       4948      20926   4.23  22.13   1.00  100.00  49.39       9.236055   \n",
       "11     143169     486639   3.58  14.67   1.28   22.82  29.63      44.793172   \n",
       "12     291763     988417   1.55   9.36   0.69   12.86  23.09      48.970065   \n",
       "13      27823      96734   2.31  11.56   1.48   21.49  25.72      46.079203   \n",
       "14      29767     102406   2.32  12.77   1.17   29.39  25.36      46.167325   \n",
       "15     475341    1641570   1.69   9.68   1.37    3.23  20.47      51.287538   \n",
       "16      21436      72564   5.06  20.08   3.12   22.03  40.10      27.272727   \n",
       "17      83027     272711   1.86   8.43   1.69   12.52  22.24      50.719377   \n",
       "18       5975      18912   2.49  11.38   3.65   12.03  27.46      42.318841   \n",
       "19      81089     287671   1.96  10.66   7.03    9.62  17.32      43.367860   \n",
       "20      18307      60171   5.06  20.77   2.22   46.34  28.16      30.413986   \n",
       "21       7000      28424  15.29  29.49  11.15  100.00  44.78       1.528571   \n",
       "22      14209      56537  11.10  23.25   2.54   26.28  54.47      15.727964   \n",
       "23      79159     283025   3.98  11.98   0.97    9.71  38.21      41.895426   \n",
       "24      11592      43879  12.80  30.29  13.47   76.18  60.34       3.651783   \n",
       "25      70615     248303   4.08  14.07   2.12   17.25  22.42      41.372758   \n",
       "26      24189      90244  10.04  26.32   3.65   52.31  56.78      12.924973   \n",
       "27      15534      58553   9.55  26.37   5.12   45.60  58.84      10.360825   \n",
       "28       7651      31230   9.67  25.12   1.02   32.95  51.36       7.672200   \n",
       "29       9307      40100  10.12  35.91   2.45   80.69  66.58       5.848833   \n",
       "...       ...        ...    ...    ...    ...     ...    ...            ...   \n",
       "2447     5818      23827   3.48  18.42   1.37   39.22  54.56      13.819182   \n",
       "2448      728       2404   5.77  30.22   0.50  100.00  54.43      17.744154   \n",
       "2449     2815       9129   6.07  30.11   1.09  100.00  50.41      14.290793   \n",
       "2450     3306      12354   2.62  16.52   0.36   42.55  34.78      23.855714   \n",
       "2451     1220       3947   6.72  34.68   0.63  100.00  41.69      13.781788   \n",
       "2452     8022      27750   6.21  26.73   0.78   40.71  46.81      19.808028   \n",
       "2453     4078      16864   6.58  24.67   2.01  100.00  54.87       5.909760   \n",
       "2454    10315      43471   6.12  20.43   3.58   48.82  60.71      11.065852   \n",
       "2455     4636      17647   6.10  26.87   1.01   69.06  50.18       7.444972   \n",
       "2456    18940      73151   8.85  24.28   0.95   91.97  60.85       3.495248   \n",
       "2457    17151      63880   4.44  22.19   0.84   47.45  50.80      20.315053   \n",
       "2458     5499      22278   5.14  23.76   1.56   76.61  65.16       6.040757   \n",
       "2459      776       2656   5.47  31.05   4.40  100.00  51.32      15.870968   \n",
       "2460    16865      62433   3.99  21.01   1.76   64.53  46.18      14.930329   \n",
       "2461      370       1329   8.07  34.51   1.96  100.00  59.53       7.027027   \n",
       "2462     4095      15484   6.42  25.03   1.24   53.26  59.53      12.576313   \n",
       "2463     2678       8855   6.03  29.96   2.51  100.00  53.95      13.757009   \n",
       "2464     2198       7071   6.96  40.89   1.94  100.00  62.78      10.191083   \n",
       "2465     1624       5378   4.05  23.72   1.51  100.00  44.32      25.862069   \n",
       "2466     7484      26645   4.34  20.09   1.11   35.68  47.96      22.893822   \n",
       "2467     4435      19413   5.64  18.13   1.41   17.20  46.50      12.051456   \n",
       "2468      908       2884   5.99  32.89   1.01  100.00  34.70      20.704846   \n",
       "2469     9049      32606   5.82  24.82   3.52   61.23  50.75      11.614543   \n",
       "2470     2379       9796   4.91  20.16   0.72  100.00  42.82      12.232030   \n",
       "2471     9496      35252   5.56  26.49   2.49   68.64  56.63       7.561078   \n",
       "2472     4632      18745   3.71  17.48   1.39   64.23  50.32       8.725702   \n",
       "2473     3165      13099   5.86  25.69   0.97   52.06  51.56      18.526715   \n",
       "2474     4678      19155   8.23  23.55   2.17  100.00  63.27       7.631466   \n",
       "2475     8477      30240   4.83  27.10   0.92   58.26  53.56      12.749380   \n",
       "2476    40236     146147   1.25   8.32   0.22    6.63  28.48      55.180130   \n",
       "\n",
       "      DISP_TV_PAGA  DISP_TEL_CELULAR  DISP_TEL_FIJO  NUM_OPS     DENS_HOGS  \\\n",
       "0        51.496795         88.942299      44.052334        1  19569.745531   \n",
       "1        38.045796         71.474924      16.288125        1   1969.400982   \n",
       "2        34.236905         71.798673      37.978258        1   1520.466582   \n",
       "3        38.846263         74.140381      15.913612        0   2712.347772   \n",
       "4        26.558655         71.472518      10.664479        0    956.898410   \n",
       "5        46.161039         88.651082      38.552381        1   5723.363820   \n",
       "6        43.179497         81.688179      27.569876        0   5440.812408   \n",
       "7        35.049521         76.393550      21.838422        1   3248.983281   \n",
       "8        48.564348         90.146983      16.296360        1   8415.414480   \n",
       "9        43.503094         78.962399      23.845788        0    242.762695   \n",
       "10       32.417138         72.797090      14.207761        0   2132.115310   \n",
       "11       66.046890         90.681400      38.056382        1    269.424652   \n",
       "12       51.078857         91.682314      44.351463        1   1863.894193   \n",
       "13       67.314825         89.086209      33.573115        1   5560.818643   \n",
       "14       63.332761         90.458684      33.829239        1   1107.720590   \n",
       "15       57.390643         91.136289      41.580929        1  38497.254483   \n",
       "16       57.697344         86.901422      28.951553        1    116.935394   \n",
       "17       63.795994         93.892835      46.692555        1    524.387787   \n",
       "18       61.892583         86.717818      35.754476        0    129.107677   \n",
       "19       59.934255         94.533275      31.624388        1   2161.768038   \n",
       "20       70.102809         87.497938      32.871516        0     57.105621   \n",
       "21       48.685714         41.700000       2.700000        0     50.044790   \n",
       "22       73.979161         68.635596      11.531963        0    675.750226   \n",
       "23       61.216034         89.157266      31.425359        1   2440.173983   \n",
       "24       50.086535         39.728280      10.756317        0    204.461032   \n",
       "25       69.532724         90.396370      32.264057        1    819.464769   \n",
       "26       69.137232         74.360989      12.188767        1    352.842844   \n",
       "27       41.411082         74.117268      14.536082        0    324.755555   \n",
       "28       70.186904         65.952163       6.090707        1    600.092551   \n",
       "29       59.918288         49.199011       9.332330        0    119.629095   \n",
       "...            ...               ...            ...      ...           ...   \n",
       "2447     31.024407         75.782056      32.640083        1    526.120651   \n",
       "2448     31.086657         74.277854      37.826685        0    449.910389   \n",
       "2449     44.756488         65.908283      40.028439        0    175.057834   \n",
       "2450     42.194604         75.113671      40.375871        1   1827.326995   \n",
       "2451     32.403610         61.033634      40.196883        0    225.821379   \n",
       "2452     30.865121         73.186238      35.078534        0    914.031789   \n",
       "2453     36.145169         67.312408      18.391368        0    998.335292   \n",
       "2454     31.917370         67.723790      25.545534        1   1599.696034   \n",
       "2455     17.350022         66.465257      15.753129        0    790.950813   \n",
       "2456     25.781415         50.786695      17.861668        0    597.695056   \n",
       "2457     38.920653         70.490082      40.752625        0    932.778594   \n",
       "2458     26.109898         43.067686      36.208151        0    388.627401   \n",
       "2459     24.774194         33.935484      36.903226        1    278.785701   \n",
       "2460     32.428106         68.069967      28.787430        1    468.190008   \n",
       "2461     23.513514         33.243243      56.216216        0    185.370741   \n",
       "2462     41.318681         66.764347      34.554335        0    998.317853   \n",
       "2463     36.785047         68.598131      27.327103        0    491.881566   \n",
       "2464     31.710646         50.318471      44.040036        0    303.369081   \n",
       "2465     45.258621         65.086207      43.103448        0    239.009816   \n",
       "2466     38.874030         77.493982      27.641081        0   1003.270953   \n",
       "2467     32.408034         67.682239      18.663958        0   2008.514107   \n",
       "2468     41.409692         66.299559      47.907489        0    295.659536   \n",
       "2469     35.794010         54.834788      34.047961        0    158.521887   \n",
       "2470     24.506095         70.701976      24.127785        0   1487.153841   \n",
       "2471     23.631003         47.335720      26.116259        0    144.350771   \n",
       "2472     39.546436         68.466523      18.444924        0   1355.416398   \n",
       "2473     29.465697         62.282643      33.797028        0    732.740658   \n",
       "2474     26.742198         64.920906      18.982471        0   1246.270247   \n",
       "2475     38.944635         65.033644      37.374572        1    389.003102   \n",
       "2476     59.844360         88.538326      48.976903        1   9113.063961   \n",
       "\n",
       "      ANOS_PROMEDIO_DE_ESCOLARIDAD  INGRESOPC_ANUAL  \n",
       "0                             9.96          4015.86  \n",
       "1                             7.04          1940.21  \n",
       "2                             6.71          2054.38  \n",
       "3                             7.89          2303.15  \n",
       "4                             7.05          1881.66  \n",
       "5                             9.18          3949.53  \n",
       "6                             8.65          3102.83  \n",
       "7                             8.37          2471.99  \n",
       "8                             8.38          2717.38  \n",
       "9                             7.95          2272.73  \n",
       "10                            7.24          2176.88  \n",
       "11                            9.21          3554.26  \n",
       "12                            9.83          4124.40  \n",
       "13                            8.81          3365.72  \n",
       "14                            8.92          3898.34  \n",
       "15                            9.47          3936.20  \n",
       "16                            8.08          3347.45  \n",
       "17                           10.58          4756.75  \n",
       "18                            9.84          3251.32  \n",
       "19                            9.56          4450.27  \n",
       "20                            7.91          3633.64  \n",
       "21                            5.59          1575.60  \n",
       "22                            8.06          2595.79  \n",
       "23                            9.95          3951.33  \n",
       "24                            6.02          1778.43  \n",
       "25                            9.73          4743.36  \n",
       "26                            7.06          2354.83  \n",
       "27                            6.81          2344.17  \n",
       "28                            8.01          2472.95  \n",
       "29                            6.06          1815.79  \n",
       "...                            ...              ...  \n",
       "2447                          7.13          1995.93  \n",
       "2448                          6.34          1733.28  \n",
       "2449                          6.44          1862.76  \n",
       "2450                          8.30          2638.00  \n",
       "2451                          5.94          1688.83  \n",
       "2452                          6.78          2036.11  \n",
       "2453                          6.52          1376.03  \n",
       "2454                          7.28          1890.25  \n",
       "2455                          6.37          1875.11  \n",
       "2456                          6.63          1322.76  \n",
       "2457                          7.71          2383.34  \n",
       "2458                          6.68          1491.48  \n",
       "2459                          6.56          1784.23  \n",
       "2460                          7.41          2159.15  \n",
       "2461                          5.72          1494.34  \n",
       "2462                          6.69          2008.35  \n",
       "2463                          6.79          1955.99  \n",
       "2464                          5.56          1595.15  \n",
       "2465                          7.61          2291.71  \n",
       "2466                          7.79          2414.92  \n",
       "2467                          7.26          2091.50  \n",
       "2468                          6.34          2068.81  \n",
       "2469                          6.89          1758.90  \n",
       "2470                          7.31          1874.56  \n",
       "2471                          6.49          1696.56  \n",
       "2472                          7.90          2249.19  \n",
       "2473                          6.99          1821.73  \n",
       "2474                          6.73          1519.50  \n",
       "2475                          6.61          1954.33  \n",
       "2476                         10.91          4220.58  \n",
       "\n",
       "[2446 rows x 15 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codigo implementado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El archivo **modelos.py** implementa tanto la regresión multinomial como diferentes redes neronales.\n",
    "\n",
    "\n",
    "Antes de proseguir en los argumentos de solución a esta tarea, cabe destacar que a efecto de extender el código proporcionado para las redes reciém presentadas para todas las redes que se abordan en esta tarea, al código de **modelos.py** se añadieron clases que permiten representar la regresión logística multinomila bajo los métodos de optimización SGD y Adam, así como las siguientes redes 1) RedNeuronal: Una capa escondida con 500 unidades (la que ya tenemos),  2) RedNeuronal3: Tres capas escondidas de 500, 100 y 30 unidades, 3) RedNeuronal5: Cinco capas escondidas de 500, 300, 100, 50, 30 unidades, considerando las posibles variaciones de estas al combinarlas con funciones de activación de tipo sigmoide y ReLU, así como cambiando los métodos de optimización empleados para poder escoger entre SGD y Adam.\n",
    "\n",
    "Al respecto, la ampliación del archivo **modelos.py** es la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# ==================\n",
    "# Regresion logistica multinomial\n",
    "# ==================\n",
    "class RegresionMultinomial(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(RegresionMultinomial, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "# ==================\n",
    "# Red neuronal de 1 capa escondida con activaciones Sigm\n",
    "# ==================\n",
    "class RedNeuronal_Sigm(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        # Permite que la clase  RedNeuronal herede los atributos y metodos de\n",
    "        # las clases hijas de la clase nn.Module al ser construida\n",
    "        super(RedNeuronal_Sigm, self).__init__()\n",
    "\n",
    "        # unidad de capa escondida\n",
    "        hidden_size = 500\n",
    "\n",
    "        # funciones de pre-activacion y activacion en capa escondida (linea-sigmoide-lineal)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.sigmoid(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# ==================\n",
    "# Red neuronal de 1 capa escondida con activaciones ReLU\n",
    "# ==================\n",
    "class RedNeuronal_ReLU(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        # Permite que la clase  RedNeuronal herede los atributos y metodos de\n",
    "        # las clases hijas de la clase nn.Module al ser construida\n",
    "        super(RedNeuronal_ReLU, self).__init__()\n",
    "\n",
    "        # unidad de capa escondida\n",
    "        hidden_size = 500\n",
    "\n",
    "        # funciones de pre-activacion y activacion en capa escondida (linea-sigmoide-lineal)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out \n",
    "    \n",
    "# ==================\n",
    "# Red neuronal de 3 capas escondidas con activaciones Sigm\n",
    "# ==================\n",
    "class RedNeuronal3_Sigm(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        # Permite que la clase  RedNeuronal herede los atributos y metodos de\n",
    "        # las clases hijas de la clase nn.Module al ser construida\n",
    "        super(RedNeuronal3_Sigm, self).__init__()\n",
    "\n",
    "        # unidades de capas escondidas\n",
    "        hidden_size1, hidden_size2, hidden_size3  = 500, 100, 30\n",
    "\n",
    "        # funciones de pre-activacion y activacion en capa escondida (linea-sigmoide-lineal)\n",
    "        self.fc1= nn.Linear(input_size, hidden_size1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.fc4 = nn.Linear(hidden_size3, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.sigmoid(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.sigmoid(out)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "# ==================\n",
    "# Red neuronal de 3 capas escondidas con activaciones ReLU\n",
    "# ==================\n",
    "class RedNeuronal3_ReLU(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        # Permite que la clase  RedNeuronal herede los atributos y metodos de\n",
    "        # las clases hijas de la clase nn.Module al ser construida\n",
    "        super(RedNeuronal3_ReLU, self).__init__()\n",
    "\n",
    "        # unidades de capas escondidas\n",
    "        hidden_size1, hidden_size2, hidden_size3  = 500, 100, 30\n",
    "\n",
    "        # funciones de pre-activacion y activacion en capa escondida (linea-sigmoide-lineal)\n",
    "        self.fc1= nn.Linear(input_size, hidden_size1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.fc4 = nn.Linear(hidden_size3, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "\n",
    "# ==================\n",
    "# Red neuronal de 5 capas escondidas con activaciones Sigm\n",
    "# ==================\n",
    "class RedNeuronal5_Sigm(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "    # Permite que la clase  RedNeuronal herede los atributos y metodos de\n",
    "    # las clases hijas de la clase nn.Module al ser construida\n",
    "        super(RedNeuronal5_Sigm, self).__init__()\n",
    "\n",
    "        # unidades de capas escondidas\n",
    "        hidden_size1, hidden_size2, hidden_size3,hidden_size4, hidden_size5  = 500, 300, 100, 50, 30\n",
    "\n",
    "        # funciones de pre-activacion y activacion en capa escondida\n",
    "        self.fc1, self.sigmoid = nn.Linear(input_size, hidden_size1), nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.fc4 = nn.Linear(hidden_size3, hidden_size4)\n",
    "        self.fc5 = nn.Linear(hidden_size4,  hidden_size5)\n",
    "        self.fc6 = nn.Linear(hidden_size5, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.sigmoid(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.sigmoid(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.sigmoid(out)\n",
    "        out = self.fc5(out)\n",
    "        out = self.sigmoid(out)\n",
    "        out = self.fc6(out)\n",
    "        return out\n",
    "\n",
    "# ==================\n",
    "# Red neuronal de 5 capas escondidas con activaciones ReLU\n",
    "# ==================\n",
    "class RedNeuronal5_ReLU(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "    # Permite que la clase  RedNeuronal herede los atributos y metodos de\n",
    "    # las clases hijas de la clase nn.Module al ser construida\n",
    "        super(RedNeuronal5_ReLU, self).__init__()\n",
    "\n",
    "        # unidades de capas escondidas\n",
    "        hidden_size1, hidden_size2, hidden_size3,hidden_size4, hidden_size5  = 500, 300, 100, 50, 30\n",
    "\n",
    "        # funciones de pre-activacion y activacion en capa escondida\n",
    "        self.fc1, self.relu = nn.Linear(input_size, hidden_size1), nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.fc4 = nn.Linear(hidden_size3, hidden_size4)\n",
    "        self.fc5 = nn.Linear(hidden_size4,  hidden_size5)\n",
    "        self.fc6 = nn.Linear(hidden_size5, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc5(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc6(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código anterior, se usó como base para poder definir una función que nos permite cambiar fácilmente el tipo de red a considerar (de entre una regresión logística multinomial, una red de una capa, un red de tres capas o una de 5), variando la función de activación (entre una de tipo sigmoide o una ReLU), así como el método de optimización a emplear (basado en SGD o Adam de Pytorch).\n",
    "\n",
    "Al respecto, también se incorporó al método SummaryWriter de Tensorboard de manera que pudiera guardar los archivos de las simulaciones a realizarse y generar dashboard para posterior análisis, quedano como sigue:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importamos el conjunto general de librerias a emplear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST # Dataset de digitos\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.tensorboard import SummaryWriter # Utilidad de tensorboard para generar graficos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importamos el archivo donde se definieron los modelos de redes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelos import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función para correr los diferentes modelos de redes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunModel(modelo, activacion, optimization_metod, directorio=\"runs/testing\", saving_model = False):\n",
    "    '''    \n",
    "    modelo:str red neuronal a considerar (\"RN1\",\"RN3\",\"RN5\",Reg. Lin. Multi:\"RLM\")        \n",
    "    activacion:str funcion de activacion de las capas de red neuronal a considerar\n",
    "    (\"Sigm\": funcion sigmoide, \"ReLU\": funcion ReLU)\n",
    "    optimization_metod:str Metodo de optimizacion para estimar parametros de la red\n",
    "        (\"SGD\": descenso de gradiente estocatico, \"Adam\": metodo Adam)\n",
    "    directorio:str especificacion del directorio para que tensorboard guarde datos para dashboards\n",
    "    saving_model:Bool especifica si el modelo se debe guardar tras correr simulacion\n",
    "    '''\n",
    "    \n",
    "    # Directorio donde se guardaran objetos de tensorboard para generar dashboard de variables monitoreadas\n",
    "    writer = SummaryWriter(directorio)\n",
    "    running_loss=0\n",
    "\n",
    "    #\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Hiper-parametros\n",
    "    input_size = 15  \n",
    "    num_classes = 6  # \n",
    "    num_epochs = 5  # Numero de epocas para entrenar\n",
    "    bs = 100  # Tamano de lote (batch_size)\n",
    "    lr = 0.001  # Tasa de aprendizaje\n",
    "\n",
    "    #\n",
    "    root = './datos'\n",
    "    # Carpeta donde se guardaran los datos\n",
    "    train_x = torch.tensor(X_train.values)#MNIST(root, train=True, transform=ToTensor(), download=True)\n",
    "    test_x = torch.tensor(X_test.values)#MNIST(root, train=False, transform=ToTensor())\n",
    "\n",
    "    #\n",
    "    train_y = torch.tensor(y_train.values)#DataLoader(train_data, batch_size=bs, shuffle=True)\n",
    "    test_y = torch.tensor(y_test.values)#DataLoader(test_data, batch_size=bs, shuffle=False)\n",
    "\n",
    "    # ==================\n",
    "    # Definimos modelo\n",
    "    # ==================\n",
    "    # Definicion del modelo de red\n",
    "    if modelo == \"RLM\":    \n",
    "        model = RegresionMultinomial(input_size, num_classes).to(device)\n",
    "    if modelo == \"RN1\"  and activacion == \"ReLU\":    \n",
    "        model = RedNeuronal_ReLU(input_size, num_classes).to(device)\n",
    "    if modelo == \"RN1\"  and activacion == \"Sigm\":    \n",
    "        model = RedNeuronal_Sigm(input_size, num_classes).to(device)\n",
    "    if modelo == \"RN3\"  and activacion == \"ReLU\":    \n",
    "        model = RedNeuronal3_ReLU(input_size, num_classes).to(device)\n",
    "    if modelo == \"RN3\"  and activacion == \"Sigm\":    \n",
    "        model = RedNeuronal3_Sigm(input_size, num_classes).to(device)\n",
    "    if modelo == \"RN5\"  and activacion == \"ReLU\":    \n",
    "        model = RedNeuronal5_ReLU(input_size, num_classes).to(device)\n",
    "    if modelo == \"RN5\"  and activacion == \"Sigm\":    \n",
    "        model = RedNeuronal5_Sigm(input_size, num_classes).to(device)\n",
    "        \n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # ==================\n",
    "    # Optimizacion\n",
    "    # ==================\n",
    "    # \n",
    "    if optimization_metod==\"SGD\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    if optimization_metod==\"Adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "    # Entrenamiento del modelo\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(train_x.size()[0]):\n",
    "            \n",
    "            xi = train_x[i].type('torch.FloatTensor')\n",
    "            yi = train_y[i]#.type('torch.FloatTensor')\n",
    "            # \n",
    "            xi = xi.to(device)  # imagenes\n",
    "            yi = yi.to(device)  # etiquetas\n",
    "\n",
    "            # Propagacion para adelante\n",
    "            output = model(xi)\n",
    "            loss = loss_function(output, yi)\n",
    "\n",
    "            # Propagcion para atras y paso de optimizacion\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss = loss.item()\n",
    "            if (i+1) % 100 == 0:\n",
    "                writer.add_scalar('training loss',running_loss/1000, epoch*len(train_loader)+1)\n",
    "                print('Epoca: {}/{}, Paso: {}/{}, Perdida: {:.5f}'.format(epoch +\n",
    "                                                                          1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    "\n",
    "    # Prueba del modelo\n",
    "    # Al probar, usamos torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for i in range(test_x.size()[0]):\n",
    "            \n",
    "            #\n",
    "            xi = test_x[i].type('torch.FloatTensor')\n",
    "            yi = test_y[i]#.type('torch.FloatTensor')\n",
    "            \n",
    "            # \n",
    "            xi = xi.to(device)\n",
    "            yi = yi.to(device)\n",
    "\n",
    "            # \n",
    "            output = model(xi)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "            total += yi.size(0)\n",
    "            correct += (predicted == yi).sum().item()\n",
    "\n",
    "            writer.add_scalar('precision_modelo',correct/100, total)\n",
    "\n",
    "        print(f'Precision del modelo en {total} imagenes: {100 * correct / total}')\n",
    "\n",
    "\n",
    "    # \n",
    "    save_model = saving_model\n",
    "    if save_model is True:\n",
    "        torch.save(model.state_dict(), 'modelo.ckpt')\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresión logística multinomial + SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-344e37560bcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRunModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RLM\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Sigm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SGD\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'runs/RLM_SGD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-75-9a8a823a2696>\u001b[0m in \u001b[0;36mRunModel\u001b[0;34m(modelo, activacion, optimization_metod, directorio, saving_model)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;31m# Propagacion para adelante\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;31m# Propagcion para atras y paso de optimizacion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1993\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log_softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "RunModel(\"RLM\", \"Sigm\", \"SGD\", 'runs/RLM_SGD', saving_model = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El error de predicción se puede calcular como $(100- 82.85)/100 \\times 100 \\% = 17.15\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.6\n",
    "\n",
    "Incluya en su archivo modelos.py la siguiente clase que implementa una red neuronal con una capa escondida y activaciones sigmoide. Rellene los comentarios;\n",
    "\n",
    "\n",
    "La implementación realizada junto con los comentarios correspondientes se pueden consultar en la ampliación del código de **modelos.py** vista en el ejercicio **2.5**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.7\n",
    "\n",
    "Pruebe el desempeño de RedNeuronal y compare los resultados con RegresionMultinomial.\n",
    "\n",
    "Al respecto, la implementación correspondiente se refiere al uso de la función presentada en 2.5 para laed Neuronal de una capa escondida, con activación de tipo sigmoide y método de optimización SGD, cuyos resultados se presentan en seguida:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Red Neuronal de una capa escondida + Activaciones Sigmoide + SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Double but got scalar type Float for argument #2 'mat2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6a76d664eb01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRunModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RN1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Sigm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SGD\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'runs/RN1_Sigm_SGD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-1de80b4c2caa>\u001b[0m in \u001b[0;36mRunModel\u001b[0;34m(modelo, activacion, optimization_metod, directorio, saving_model)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;31m# Propagacion para adelante\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/Box/Aprendizaje_Maquina/Projecto/modelos.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Double but got scalar type Float for argument #2 'mat2'"
     ]
    }
   ],
   "source": [
    "RunModel(\"RN1\", \"Sigm\", \"SGD\", 'runs/RN1_Sigm_SGD', saving_model = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manera análoga al ejercicio anterior, ello implica un error de predicción de 55.89%, el cual es casi tres veces mayor al obtenido en el caso de la regresión logística multinomial presentando previamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.8\n",
    "\n",
    "a) Genere gráficas que muestren el decremento en la función de pérdida y en la precisión sobre los valores de prueba (Puede usar matplotlib o tensorboard). Comente sus observaciones.\n",
    "\n",
    "Para dar respuesta a este ejercicio, se corrieron el resto de modelos correspondientes a las redes RedNeuronal, RedNeuronal3 y RedNeuronal5, variando el método de optimización junto con la función de activación. Los resultados obtenidos se pueden revisar en el Anexo al final del presente documento.\n",
    "\n",
    "Ahora bien, a efecto de falicitar el entendimiento de los siguientes gráficos, se presenta una tabla que resume la notación empleada para nombrar cada modelo de red neuronal, según su tipo, función de activación y método de optimizacion:\n",
    "\n",
    "| Modelo             | Activaciones | Optimizador | Notación |\n",
    "|--------------------|--------------|-----------------|------------------|\n",
    "| RedNeuronal        | ReLU         | SGD       |      RN_ReLU_SGD      |\n",
    "| RedNeuronal        | ReLU         | Adam      |       RN_ReLU_Adam    |\n",
    "| RedNeuronal        | Sigmoid      | SGD       |     RN_Sigm_SGD       |\n",
    "| RedNeuronal        | Sigmoid      | Adam      |       RN_Sigm_Adam    |\n",
    "| RedNeuronal3        | ReLU         | SGD       |      RN3_ReLU_SGD      |\n",
    "| RedNeuronal3        | ReLU         | Adam      |       RN3_ReLU_Adam    |\n",
    "| RedNeuronal3        | Sigmoid      | SGD       |     RN3_Sigm_SGD       |\n",
    "| RedNeuronal3        | Sigmoid      | Adam      |       RN3_Sigm_Adam    |\n",
    "| RedNeuronal5        | ReLU         | SGD       |      RN5_ReLU_SGD      |\n",
    "| RedNeuronal5       | ReLU         | Adam      |       RN5_ReLU_Adam    |\n",
    "| RedNeuronal5        | Sigmoid      | SGD       |     RN5_Sigm_SGD       |\n",
    "| RedNeuronal5        | Sigmoid      | Adam      |       RN5_Sigm_Adam    |\n",
    "\n",
    "**Pérdidas a lo largo de las épocas**\n",
    "\n",
    "A continuación se muestran las gráficas la perdida obtenidas a lo largo de las diferentes épocas. Se puede apreciar que el modelo RN5\\_ReLU\\_Adam fue el que observó un pérdida más baja en las iteraciones consideradas:\n",
    "\n",
    "![Error de entrenamiento a lo largo de las épocas](images/loss.png)\n",
    "![Error de entrenamiento a lo largo de las épocas](images/loss_table.png)\n",
    "\n",
    "Cabe apreciar que los modelos de la RedNeuronal basados en SGD tuvieron las pérdidas más bajas.\n",
    "\n",
    "**Precisión del modelo lo largo de las épocas**\n",
    "\n",
    "A continuación se muestra las gráficas la precisiṕn obtenida pr el modelo al evaluar la predicción de cada elemento del conjunto de prueba contra el dígito correspondiente. Se puede apreciar que el modelo RN1\\_ReLU\\_Adam fue con la mejor precisión (97.94%):\n",
    "\n",
    "![Error de entrenamiento a lo largo de las épocas](images/precision.png)\n",
    "![Error de entrenamiento a lo largo de las épocas](images/precision_table.png)\n",
    "\n",
    "En adición, cabe apreciar que los models de RedNeuronal3 y RedNeuronal5 basados función de activación sigmoide y método de optimización SGD tuvieron la peor predicción con 11.35%.\n",
    "\n",
    "Para complementar este punto, a continuación se presenta un tabla resumen de los valores obtenidos para la precisión en los distintos modelos:\n",
    "\n",
    "**Precisión obtenida con los diferentes modelos**\n",
    "\n",
    "| Modelo             | Activaciones | torch.optim.SGD | torch.optim.Adam |\n",
    "|--------------------|--------------|-----------------|------------------|\n",
    "| RegresionLogistica | -            | 82.85%           | 92.26%            |\n",
    "| RedNeuronal        | ReLU         | 44.11%          | 96.75%            |\n",
    "| RedNeuronal        | Sigmoid      | 78.66%           | 97.74%            |\n",
    "| RedNeuronal3       | ReLU         | 11.35%           | 97.27%            |\n",
    "| RedNeuronal3       | Sigmoid      | 35.21%           | 97.76%            |\n",
    "| RedNeuronal5       | ReLU         | 11.35%           | 97.02%            |\n",
    "| RedNeuronal5       | Sigmoid      | 11.35%           | 97.47%            |\n",
    "\n",
    "Se puede observar como, en términos generales, los modelos de redes neuronales basados en el método de optimización Adam arrojan una precisión alta para este problema de clasificación, independientemente de la función de activación considerada, en contraste con aquellos basados en SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Complete las siguientes tablas.\n",
    "\n",
    "**Error de predicción**\n",
    "\n",
    "\n",
    "|Modelo             | Activaciones | torch.optim.SGD | torch.optim.Adam |\n",
    "|--------------------|--------------|-----------------|------------------|\n",
    "| RegresionLogistica | -            | 17.15%           | 7.74%             |\n",
    "| RedNeuronal        | ReLU         | 57.89%           | 3.25%             |\n",
    "| RedNeuronal        | Sigmoid      | 21.34%           | 2.26%             |\n",
    "| RedNeuronal3       | ReLU         | 88.65%           | 2.73%             |\n",
    "| RedNeuronal3       | Sigmoid      | 64.79%           | 2.24%             |\n",
    "| RedNeuronal5       | ReLU         | 88.65%           | 2.98%             |\n",
    "| RedNeuronal5       | Sigmoid      | 88.65%           | 2.53%             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.9. \n",
    "\n",
    "¿Qué puede concluir a partir de estos experimentos?\n",
    "\n",
    "Derivado del ejercicio anterior, se pueden presumir los siguientes puntos:\n",
    "\n",
    "* En este problema particular de clasificación, aparentemente Adam es una mejor opción de método de optimización de SGD pues arroja errores de predicción muy bajos al compararse con los obtenidos contra métodos basados en la misma red pero que usan SGD.\n",
    "* Asimismo, la red predice mejor el problema de clasificación de imágenes de dígitos cuando se emplean ReLU que cuando se echa mano de Sigmoid, restringidos al optimizador SGD.\n",
    "* Dicha tendencia se invierte si el método de optimización es Adam.\n",
    "* El desempeño de la predicción con regresión logística multinomial en superado por más de la mitad por los métodos basados en redes neuronales en aquellos casos en los que se usa a Adam como optimizador de los parámetros de la red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anexo de resultados de resto de modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: 1/5, Paso: 100/600, Perdida: 0.84714\n",
      "Epoca: 1/5, Paso: 200/600, Perdida: 0.55226\n",
      "Epoca: 1/5, Paso: 300/600, Perdida: 0.43952\n",
      "Epoca: 1/5, Paso: 400/600, Perdida: 0.58421\n",
      "Epoca: 1/5, Paso: 500/600, Perdida: 0.37473\n",
      "Epoca: 1/5, Paso: 600/600, Perdida: 0.44304\n",
      "Epoca: 2/5, Paso: 100/600, Perdida: 0.44089\n",
      "Epoca: 2/5, Paso: 200/600, Perdida: 0.43384\n",
      "Epoca: 2/5, Paso: 300/600, Perdida: 0.36927\n",
      "Epoca: 2/5, Paso: 400/600, Perdida: 0.28155\n",
      "Epoca: 2/5, Paso: 500/600, Perdida: 0.23258\n",
      "Epoca: 2/5, Paso: 600/600, Perdida: 0.31027\n",
      "Epoca: 3/5, Paso: 100/600, Perdida: 0.26544\n",
      "Epoca: 3/5, Paso: 200/600, Perdida: 0.41425\n",
      "Epoca: 3/5, Paso: 300/600, Perdida: 0.41398\n",
      "Epoca: 3/5, Paso: 400/600, Perdida: 0.28983\n",
      "Epoca: 3/5, Paso: 500/600, Perdida: 0.31366\n",
      "Epoca: 3/5, Paso: 600/600, Perdida: 0.31253\n",
      "Epoca: 4/5, Paso: 100/600, Perdida: 0.42190\n",
      "Epoca: 4/5, Paso: 200/600, Perdida: 0.39038\n",
      "Epoca: 4/5, Paso: 300/600, Perdida: 0.39345\n",
      "Epoca: 4/5, Paso: 400/600, Perdida: 0.33981\n",
      "Epoca: 4/5, Paso: 500/600, Perdida: 0.28323\n",
      "Epoca: 4/5, Paso: 600/600, Perdida: 0.23854\n",
      "Epoca: 5/5, Paso: 100/600, Perdida: 0.28036\n",
      "Epoca: 5/5, Paso: 200/600, Perdida: 0.33646\n",
      "Epoca: 5/5, Paso: 300/600, Perdida: 0.26711\n",
      "Epoca: 5/5, Paso: 400/600, Perdida: 0.27301\n",
      "Epoca: 5/5, Paso: 500/600, Perdida: 0.39146\n",
      "Epoca: 5/5, Paso: 600/600, Perdida: 0.22046\n",
      "Precision del modelo en 10000 imagenes: 92.26\n"
     ]
    }
   ],
   "source": [
    "RunModel(\"RLM\", \"Sigm\", \"Adam\", 'runs/RLM_Adam', saving_model = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresión logística multinomial + Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red Neuronal de una capa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Red Neuronal de una capa escondida + Activaciones ReLU + SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: 1/5, Paso: 100/600, Perdida: 2.27098\n",
      "Epoca: 1/5, Paso: 200/600, Perdida: 2.25274\n",
      "Epoca: 1/5, Paso: 300/600, Perdida: 2.21254\n",
      "Epoca: 1/5, Paso: 400/600, Perdida: 2.20317\n",
      "Epoca: 1/5, Paso: 500/600, Perdida: 2.18718\n",
      "Epoca: 1/5, Paso: 600/600, Perdida: 2.17403\n",
      "Epoca: 2/5, Paso: 100/600, Perdida: 2.10185\n",
      "Epoca: 2/5, Paso: 200/600, Perdida: 2.10962\n",
      "Epoca: 2/5, Paso: 300/600, Perdida: 2.01843\n",
      "Epoca: 2/5, Paso: 400/600, Perdida: 2.05426\n",
      "Epoca: 2/5, Paso: 500/600, Perdida: 2.01789\n",
      "Epoca: 2/5, Paso: 600/600, Perdida: 1.98587\n",
      "Epoca: 3/5, Paso: 100/600, Perdida: 1.94279\n",
      "Epoca: 3/5, Paso: 200/600, Perdida: 1.93208\n",
      "Epoca: 3/5, Paso: 300/600, Perdida: 1.88867\n",
      "Epoca: 3/5, Paso: 400/600, Perdida: 1.87086\n",
      "Epoca: 3/5, Paso: 500/600, Perdida: 1.87709\n",
      "Epoca: 3/5, Paso: 600/600, Perdida: 1.78198\n",
      "Epoca: 4/5, Paso: 100/600, Perdida: 1.82296\n",
      "Epoca: 4/5, Paso: 200/600, Perdida: 1.69206\n",
      "Epoca: 4/5, Paso: 300/600, Perdida: 1.70763\n",
      "Epoca: 4/5, Paso: 400/600, Perdida: 1.61053\n",
      "Epoca: 4/5, Paso: 500/600, Perdida: 1.68122\n",
      "Epoca: 4/5, Paso: 600/600, Perdida: 1.61961\n",
      "Epoca: 5/5, Paso: 100/600, Perdida: 1.56045\n",
      "Epoca: 5/5, Paso: 200/600, Perdida: 1.48188\n",
      "Epoca: 5/5, Paso: 300/600, Perdida: 1.45809\n",
      "Epoca: 5/5, Paso: 400/600, Perdida: 1.54109\n",
      "Epoca: 5/5, Paso: 500/600, Perdida: 1.49054\n",
      "Epoca: 5/5, Paso: 600/600, Perdida: 1.41268\n",
      "Precision del modelo en 10000 imagenes: 78.66\n"
     ]
    }
   ],
   "source": [
    "RunModel(\"RN1\", \"ReLU\", \"SGD\", 'runs/RN1_ReLU_SGD', saving_model = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Red Neuronal de una capa escondida + Activaciones Sigmoide + Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: 1/5, Paso: 100/600, Perdida: 0.63190\n",
      "Epoca: 1/5, Paso: 200/600, Perdida: 0.24995\n",
      "Epoca: 1/5, Paso: 300/600, Perdida: 0.38825\n",
      "Epoca: 1/5, Paso: 400/600, Perdida: 0.22471\n",
      "Epoca: 1/5, Paso: 500/600, Perdida: 0.31148\n",
      "Epoca: 1/5, Paso: 600/600, Perdida: 0.23046\n",
      "Epoca: 2/5, Paso: 100/600, Perdida: 0.25828\n",
      "Epoca: 2/5, Paso: 200/600, Perdida: 0.14849\n",
      "Epoca: 2/5, Paso: 300/600, Perdida: 0.20679\n",
      "Epoca: 2/5, Paso: 400/600, Perdida: 0.22641\n",
      "Epoca: 2/5, Paso: 500/600, Perdida: 0.22358\n",
      "Epoca: 2/5, Paso: 600/600, Perdida: 0.13848\n",
      "Epoca: 3/5, Paso: 100/600, Perdida: 0.13382\n",
      "Epoca: 3/5, Paso: 200/600, Perdida: 0.20473\n",
      "Epoca: 3/5, Paso: 300/600, Perdida: 0.07089\n",
      "Epoca: 3/5, Paso: 400/600, Perdida: 0.16958\n",
      "Epoca: 3/5, Paso: 500/600, Perdida: 0.10779\n",
      "Epoca: 3/5, Paso: 600/600, Perdida: 0.17531\n",
      "Epoca: 4/5, Paso: 100/600, Perdida: 0.07445\n",
      "Epoca: 4/5, Paso: 200/600, Perdida: 0.06639\n",
      "Epoca: 4/5, Paso: 300/600, Perdida: 0.15601\n",
      "Epoca: 4/5, Paso: 400/600, Perdida: 0.11596\n",
      "Epoca: 4/5, Paso: 500/600, Perdida: 0.08174\n",
      "Epoca: 4/5, Paso: 600/600, Perdida: 0.11613\n",
      "Epoca: 5/5, Paso: 100/600, Perdida: 0.06456\n",
      "Epoca: 5/5, Paso: 200/600, Perdida: 0.04687\n",
      "Epoca: 5/5, Paso: 300/600, Perdida: 0.26638\n",
      "Epoca: 5/5, Paso: 400/600, Perdida: 0.09992\n",
      "Epoca: 5/5, Paso: 500/600, Perdida: 0.06707\n",
      "Epoca: 5/5, Paso: 600/600, Perdida: 0.03704\n",
      "Precision del modelo en 10000 imagenes: 96.75\n"
     ]
    }
   ],
   "source": [
    "RunModel(\"RN1\", \"Sigm\", \"Adam\", 'runs/RN1_Sigm_Adam', saving_model = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Red Neuronal de una capa escondida + Activaciones ReLU + Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: 1/5, Paso: 100/600, Perdida: 0.40839\n",
      "Epoca: 1/5, Paso: 200/600, Perdida: 0.22708\n",
      "Epoca: 1/5, Paso: 300/600, Perdida: 0.34032\n",
      "Epoca: 1/5, Paso: 400/600, Perdida: 0.13182\n",
      "Epoca: 1/5, Paso: 500/600, Perdida: 0.29857\n",
      "Epoca: 1/5, Paso: 600/600, Perdida: 0.13923\n",
      "Epoca: 2/5, Paso: 100/600, Perdida: 0.09076\n",
      "Epoca: 2/5, Paso: 200/600, Perdida: 0.08429\n",
      "Epoca: 2/5, Paso: 300/600, Perdida: 0.04753\n",
      "Epoca: 2/5, Paso: 400/600, Perdida: 0.08848\n",
      "Epoca: 2/5, Paso: 500/600, Perdida: 0.06071\n",
      "Epoca: 2/5, Paso: 600/600, Perdida: 0.09700\n",
      "Epoca: 3/5, Paso: 100/600, Perdida: 0.07444\n",
      "Epoca: 3/5, Paso: 200/600, Perdida: 0.07097\n",
      "Epoca: 3/5, Paso: 300/600, Perdida: 0.09218\n",
      "Epoca: 3/5, Paso: 400/600, Perdida: 0.06149\n",
      "Epoca: 3/5, Paso: 500/600, Perdida: 0.07703\n",
      "Epoca: 3/5, Paso: 600/600, Perdida: 0.07854\n",
      "Epoca: 4/5, Paso: 100/600, Perdida: 0.03885\n",
      "Epoca: 4/5, Paso: 200/600, Perdida: 0.01693\n",
      "Epoca: 4/5, Paso: 300/600, Perdida: 0.04746\n",
      "Epoca: 4/5, Paso: 400/600, Perdida: 0.05456\n",
      "Epoca: 4/5, Paso: 500/600, Perdida: 0.06893\n",
      "Epoca: 4/5, Paso: 600/600, Perdida: 0.05302\n",
      "Epoca: 5/5, Paso: 100/600, Perdida: 0.04532\n",
      "Epoca: 5/5, Paso: 200/600, Perdida: 0.03250\n",
      "Epoca: 5/5, Paso: 300/600, Perdida: 0.03239\n",
      "Epoca: 5/5, Paso: 400/600, Perdida: 0.03191\n",
      "Epoca: 5/5, Paso: 500/600, Perdida: 0.02345\n",
      "Epoca: 5/5, Paso: 600/600, Perdida: 0.03931\n",
      "Precision del modelo en 10000 imagenes: 97.94\n"
     ]
    }
   ],
   "source": [
    "RunModel(\"RN1\", \"ReLU\", \"Adam\", 'runs/RN1_ReLU_Adam', saving_model = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red Neuronal de 3 capas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Red Neuronal de 3 capas escondidas + Activaciones Sigmoide + SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: 1/5, Paso: 100/600, Perdida: 2.30880\n",
      "Epoca: 1/5, Paso: 200/600, Perdida: 2.31254\n",
      "Epoca: 1/5, Paso: 300/600, Perdida: 2.33314\n",
      "Epoca: 1/5, Paso: 400/600, Perdida: 2.29346\n",
      "Epoca: 1/5, Paso: 500/600, Perdida: 2.31503\n",
      "Epoca: 1/5, Paso: 600/600, Perdida: 2.32177\n",
      "Epoca: 2/5, Paso: 100/600, Perdida: 2.30833\n",
      "Epoca: 2/5, Paso: 200/600, Perdida: 2.30469\n",
      "Epoca: 2/5, Paso: 300/600, Perdida: 2.29137\n",
      "Epoca: 2/5, Paso: 400/600, Perdida: 2.31299\n",
      "Epoca: 2/5, Paso: 500/600, Perdida: 2.30627\n",
      "Epoca: 2/5, Paso: 600/600, Perdida: 2.31453\n",
      "Epoca: 3/5, Paso: 100/600, Perdida: 2.28361\n",
      "Epoca: 3/5, Paso: 200/600, Perdida: 2.31213\n",
      "Epoca: 3/5, Paso: 300/600, Perdida: 2.29823\n",
      "Epoca: 3/5, Paso: 400/600, Perdida: 2.30357\n",
      "Epoca: 3/5, Paso: 500/600, Perdida: 2.30616\n",
      "Epoca: 3/5, Paso: 600/600, Perdida: 2.30596\n",
      "Epoca: 4/5, Paso: 100/600, Perdida: 2.30053\n",
      "Epoca: 4/5, Paso: 200/600, Perdida: 2.29734\n",
      "Epoca: 4/5, Paso: 300/600, Perdida: 2.30434\n",
      "Epoca: 4/5, Paso: 400/600, Perdida: 2.30561\n",
      "Epoca: 4/5, Paso: 500/600, Perdida: 2.29731\n",
      "Epoca: 4/5, Paso: 600/600, Perdida: 2.29751\n",
      "Epoca: 5/5, Paso: 100/600, Perdida: 2.30132\n",
      "Epoca: 5/5, Paso: 200/600, Perdida: 2.30346\n",
      "Epoca: 5/5, Paso: 300/600, Perdida: 2.29415\n",
      "Epoca: 5/5, Paso: 400/600, Perdida: 2.30277\n",
      "Epoca: 5/5, Paso: 500/600, Perdida: 2.30229\n",
      "Epoca: 5/5, Paso: 600/600, Perdida: 2.29812\n",
      "Precision del modelo en 10000 imagenes: 11.35\n"
     ]
    }
   ],
   "source": [
    "RunModel(\"RN3\", \"Sigm\", \"SGD\", 'runs/RN3_Sigm_SGD', saving_model = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Red Neuronal de 3 capas escondidas + Activaciones ReLU + SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: 1/5, Paso: 100/600, Perdida: 2.30752\n",
      "Epoca: 1/5, Paso: 200/600, Perdida: 2.29760\n",
      "Epoca: 1/5, Paso: 300/600, Perdida: 2.29984\n",
      "Epoca: 1/5, Paso: 400/600, Perdida: 2.30616\n",
      "Epoca: 1/5, Paso: 500/600, Perdida: 2.29596\n",
      "Epoca: 1/5, Paso: 600/600, Perdida: 2.29809\n",
      "Epoca: 2/5, Paso: 100/600, Perdida: 2.29873\n",
      "Epoca: 2/5, Paso: 200/600, Perdida: 2.29051\n",
      "Epoca: 2/5, Paso: 300/600, Perdida: 2.29435\n",
      "Epoca: 2/5, Paso: 400/600, Perdida: 2.28852\n",
      "Epoca: 2/5, Paso: 500/600, Perdida: 2.30028\n",
      "Epoca: 2/5, Paso: 600/600, Perdida: 2.29430\n",
      "Epoca: 3/5, Paso: 100/600, Perdida: 2.28674\n",
      "Epoca: 3/5, Paso: 200/600, Perdida: 2.28457\n",
      "Epoca: 3/5, Paso: 300/600, Perdida: 2.28827\n",
      "Epoca: 3/5, Paso: 400/600, Perdida: 2.29361\n",
      "Epoca: 3/5, Paso: 500/600, Perdida: 2.28675\n",
      "Epoca: 3/5, Paso: 600/600, Perdida: 2.28899\n",
      "Epoca: 4/5, Paso: 100/600, Perdida: 2.29439\n",
      "Epoca: 4/5, Paso: 200/600, Perdida: 2.28527\n",
      "Epoca: 4/5, Paso: 300/600, Perdida: 2.29050\n",
      "Epoca: 4/5, Paso: 400/600, Perdida: 2.27703\n",
      "Epoca: 4/5, Paso: 500/600, Perdida: 2.27671\n",
      "Epoca: 4/5, Paso: 600/600, Perdida: 2.27096\n",
      "Epoca: 5/5, Paso: 100/600, Perdida: 2.27970\n",
      "Epoca: 5/5, Paso: 200/600, Perdida: 2.28153\n",
      "Epoca: 5/5, Paso: 300/600, Perdida: 2.28522\n",
      "Epoca: 5/5, Paso: 400/600, Perdida: 2.27369\n",
      "Epoca: 5/5, Paso: 500/600, Perdida: 2.26710\n",
      "Epoca: 5/5, Paso: 600/600, Perdida: 2.26902\n",
      "Precision del modelo en 10000 imagenes: 35.21\n"
     ]
    }
   ],
   "source": [
    "RunModel(\"RN3\", \"ReLU\", \"SGD\", 'runs/RN3_ReLU_SGD', saving_model = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Red Neuronal de 3 capas escondidas + Activaciones Sigmoide + Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: 1/5, Paso: 100/600, Perdida: 1.76503\n",
      "Epoca: 1/5, Paso: 200/600, Perdida: 1.12979\n",
      "Epoca: 1/5, Paso: 300/600, Perdida: 0.88725\n",
      "Epoca: 1/5, Paso: 400/600, Perdida: 0.65549\n",
      "Epoca: 1/5, Paso: 500/600, Perdida: 0.47046\n",
      "Epoca: 1/5, Paso: 600/600, Perdida: 0.40680\n",
      "Epoca: 2/5, Paso: 100/600, Perdida: 0.32816\n",
      "Epoca: 2/5, Paso: 200/600, Perdida: 0.28952\n",
      "Epoca: 2/5, Paso: 300/600, Perdida: 0.23769\n",
      "Epoca: 2/5, Paso: 400/600, Perdida: 0.20549\n",
      "Epoca: 2/5, Paso: 500/600, Perdida: 0.32464\n",
      "Epoca: 2/5, Paso: 600/600, Perdida: 0.20629\n",
      "Epoca: 3/5, Paso: 100/600, Perdida: 0.20500\n",
      "Epoca: 3/5, Paso: 200/600, Perdida: 0.34998\n",
      "Epoca: 3/5, Paso: 300/600, Perdida: 0.12600\n",
      "Epoca: 3/5, Paso: 400/600, Perdida: 0.15098\n",
      "Epoca: 3/5, Paso: 500/600, Perdida: 0.19190\n",
      "Epoca: 3/5, Paso: 600/600, Perdida: 0.04021\n",
      "Epoca: 4/5, Paso: 100/600, Perdida: 0.05183\n",
      "Epoca: 4/5, Paso: 200/600, Perdida: 0.16688\n",
      "Epoca: 4/5, Paso: 300/600, Perdida: 0.07373\n",
      "Epoca: 4/5, Paso: 400/600, Perdida: 0.25256\n",
      "Epoca: 4/5, Paso: 500/600, Perdida: 0.13635\n",
      "Epoca: 4/5, Paso: 600/600, Perdida: 0.09976\n",
      "Epoca: 5/5, Paso: 100/600, Perdida: 0.13871\n",
      "Epoca: 5/5, Paso: 200/600, Perdida: 0.09398\n",
      "Epoca: 5/5, Paso: 300/600, Perdida: 0.06319\n",
      "Epoca: 5/5, Paso: 400/600, Perdida: 0.07715\n",
      "Epoca: 5/5, Paso: 500/600, Perdida: 0.05916\n",
      "Epoca: 5/5, Paso: 600/600, Perdida: 0.12624\n",
      "Precision del modelo en 10000 imagenes: 97.27\n"
     ]
    }
   ],
   "source": [
    "RunModel(\"RN3\", \"Sigm\", \"Adam\", 'runs/RN3_Sigm_Adam', saving_model = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Red Neuronal de 3 capas escondidas + Activaciones ReLU + Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: 1/5, Paso: 100/600, Perdida: 0.23658\n",
      "Epoca: 1/5, Paso: 200/600, Perdida: 0.13085\n",
      "Epoca: 1/5, Paso: 300/600, Perdida: 0.21071\n",
      "Epoca: 1/5, Paso: 400/600, Perdida: 0.15012\n",
      "Epoca: 1/5, Paso: 500/600, Perdida: 0.18413\n",
      "Epoca: 1/5, Paso: 600/600, Perdida: 0.08857\n",
      "Epoca: 2/5, Paso: 100/600, Perdida: 0.29261\n",
      "Epoca: 2/5, Paso: 200/600, Perdida: 0.18313\n",
      "Epoca: 2/5, Paso: 300/600, Perdida: 0.12010\n",
      "Epoca: 2/5, Paso: 400/600, Perdida: 0.09019\n",
      "Epoca: 2/5, Paso: 500/600, Perdida: 0.20964\n",
      "Epoca: 2/5, Paso: 600/600, Perdida: 0.08635\n",
      "Epoca: 3/5, Paso: 100/600, Perdida: 0.06402\n",
      "Epoca: 3/5, Paso: 200/600, Perdida: 0.13483\n",
      "Epoca: 3/5, Paso: 300/600, Perdida: 0.07353\n",
      "Epoca: 3/5, Paso: 400/600, Perdida: 0.05996\n",
      "Epoca: 3/5, Paso: 500/600, Perdida: 0.05578\n",
      "Epoca: 3/5, Paso: 600/600, Perdida: 0.09890\n",
      "Epoca: 4/5, Paso: 100/600, Perdida: 0.01791\n",
      "Epoca: 4/5, Paso: 200/600, Perdida: 0.05250\n",
      "Epoca: 4/5, Paso: 300/600, Perdida: 0.04896\n",
      "Epoca: 4/5, Paso: 400/600, Perdida: 0.10034\n",
      "Epoca: 4/5, Paso: 500/600, Perdida: 0.02419\n",
      "Epoca: 4/5, Paso: 600/600, Perdida: 0.10551\n",
      "Epoca: 5/5, Paso: 100/600, Perdida: 0.13595\n",
      "Epoca: 5/5, Paso: 200/600, Perdida: 0.04264\n",
      "Epoca: 5/5, Paso: 300/600, Perdida: 0.04766\n",
      "Epoca: 5/5, Paso: 400/600, Perdida: 0.06977\n",
      "Epoca: 5/5, Paso: 500/600, Perdida: 0.00561\n",
      "Epoca: 5/5, Paso: 600/600, Perdida: 0.03339\n",
      "Precision del modelo en 10000 imagenes: 97.76\n"
     ]
    }
   ],
   "source": [
    "RunModel(\"RN3\", \"ReLU\", \"Adam\", 'runs/RN3_ReLU_Adam', saving_model = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red Neuronal de 5 capas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Red Neuronal de 5 capas escondidas + Activaciones Sigmoide + SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: 1/5, Paso: 100/600, Perdida: 2.34314\n",
      "Epoca: 1/5, Paso: 200/600, Perdida: 2.28729\n",
      "Epoca: 1/5, Paso: 300/600, Perdida: 2.31738\n",
      "Epoca: 1/5, Paso: 400/600, Perdida: 2.33431\n",
      "Epoca: 1/5, Paso: 500/600, Perdida: 2.29466\n",
      "Epoca: 1/5, Paso: 600/600, Perdida: 2.29585\n",
      "Epoca: 2/5, Paso: 100/600, Perdida: 2.31283\n",
      "Epoca: 2/5, Paso: 200/600, Perdida: 2.30929\n",
      "Epoca: 2/5, Paso: 300/600, Perdida: 2.31403\n",
      "Epoca: 2/5, Paso: 400/600, Perdida: 2.28551\n",
      "Epoca: 2/5, Paso: 500/600, Perdida: 2.31438\n",
      "Epoca: 2/5, Paso: 600/600, Perdida: 2.29786\n",
      "Epoca: 3/5, Paso: 100/600, Perdida: 2.29774\n",
      "Epoca: 3/5, Paso: 200/600, Perdida: 2.30407\n",
      "Epoca: 3/5, Paso: 300/600, Perdida: 2.30064\n",
      "Epoca: 3/5, Paso: 400/600, Perdida: 2.30033\n",
      "Epoca: 3/5, Paso: 500/600, Perdida: 2.31409\n",
      "Epoca: 3/5, Paso: 600/600, Perdida: 2.30282\n",
      "Epoca: 4/5, Paso: 100/600, Perdida: 2.30571\n",
      "Epoca: 4/5, Paso: 200/600, Perdida: 2.29104\n",
      "Epoca: 4/5, Paso: 300/600, Perdida: 2.30091\n",
      "Epoca: 4/5, Paso: 400/600, Perdida: 2.29528\n",
      "Epoca: 4/5, Paso: 500/600, Perdida: 2.30531\n",
      "Epoca: 4/5, Paso: 600/600, Perdida: 2.29843\n",
      "Epoca: 5/5, Paso: 100/600, Perdida: 2.30664\n",
      "Epoca: 5/5, Paso: 200/600, Perdida: 2.30174\n",
      "Epoca: 5/5, Paso: 300/600, Perdida: 2.29718\n",
      "Epoca: 5/5, Paso: 400/600, Perdida: 2.30193\n",
      "Epoca: 5/5, Paso: 500/600, Perdida: 2.29760\n",
      "Epoca: 5/5, Paso: 600/600, Perdida: 2.31084\n",
      "Precision del modelo en 10000 imagenes: 11.35\n"
     ]
    }
   ],
   "source": [
    "RunModel(\"RN5\", \"Sigm\", \"SGD\", 'runs/RN5_Sigm_SGD', saving_model = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Red Neuronal de 5 capas escondidas + Activaciones ReLU + SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: 1/5, Paso: 100/600, Perdida: 2.29845\n",
      "Epoca: 1/5, Paso: 200/600, Perdida: 2.30885\n",
      "Epoca: 1/5, Paso: 300/600, Perdida: 2.29811\n",
      "Epoca: 1/5, Paso: 400/600, Perdida: 2.31248\n",
      "Epoca: 1/5, Paso: 500/600, Perdida: 2.30376\n",
      "Epoca: 1/5, Paso: 600/600, Perdida: 2.30326\n",
      "Epoca: 2/5, Paso: 100/600, Perdida: 2.30227\n",
      "Epoca: 2/5, Paso: 200/600, Perdida: 2.30039\n",
      "Epoca: 2/5, Paso: 300/600, Perdida: 2.31407\n",
      "Epoca: 2/5, Paso: 400/600, Perdida: 2.30311\n",
      "Epoca: 2/5, Paso: 500/600, Perdida: 2.30233\n",
      "Epoca: 2/5, Paso: 600/600, Perdida: 2.30865\n",
      "Epoca: 3/5, Paso: 100/600, Perdida: 2.30527\n",
      "Epoca: 3/5, Paso: 200/600, Perdida: 2.29979\n",
      "Epoca: 3/5, Paso: 300/600, Perdida: 2.31136\n",
      "Epoca: 3/5, Paso: 400/600, Perdida: 2.30690\n",
      "Epoca: 3/5, Paso: 500/600, Perdida: 2.31134\n",
      "Epoca: 3/5, Paso: 600/600, Perdida: 2.30824\n",
      "Epoca: 4/5, Paso: 100/600, Perdida: 2.30211\n",
      "Epoca: 4/5, Paso: 200/600, Perdida: 2.30509\n",
      "Epoca: 4/5, Paso: 300/600, Perdida: 2.31265\n",
      "Epoca: 4/5, Paso: 400/600, Perdida: 2.30074\n",
      "Epoca: 4/5, Paso: 500/600, Perdida: 2.30944\n",
      "Epoca: 4/5, Paso: 600/600, Perdida: 2.30186\n",
      "Epoca: 5/5, Paso: 100/600, Perdida: 2.29582\n",
      "Epoca: 5/5, Paso: 200/600, Perdida: 2.29964\n",
      "Epoca: 5/5, Paso: 300/600, Perdida: 2.30309\n",
      "Epoca: 5/5, Paso: 400/600, Perdida: 2.29209\n",
      "Epoca: 5/5, Paso: 500/600, Perdida: 2.30024\n",
      "Epoca: 5/5, Paso: 600/600, Perdida: 2.30000\n",
      "Precision del modelo en 10000 imagenes: 11.35\n"
     ]
    }
   ],
   "source": [
    "RunModel(\"RN5\", \"ReLU\", \"SGD\", 'runs/RN5_ReLU_SGD', saving_model = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Red Neuronal de 5 capas escondidas + Activaciones Sigmoide + Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: 1/5, Paso: 100/600, Perdida: 1.75372\n",
      "Epoca: 1/5, Paso: 200/600, Perdida: 1.22390\n",
      "Epoca: 1/5, Paso: 300/600, Perdida: 0.86268\n",
      "Epoca: 1/5, Paso: 400/600, Perdida: 0.72304\n",
      "Epoca: 1/5, Paso: 500/600, Perdida: 0.40169\n",
      "Epoca: 1/5, Paso: 600/600, Perdida: 0.43360\n",
      "Epoca: 2/5, Paso: 100/600, Perdida: 0.26429\n",
      "Epoca: 2/5, Paso: 200/600, Perdida: 0.23229\n",
      "Epoca: 2/5, Paso: 300/600, Perdida: 0.30579\n",
      "Epoca: 2/5, Paso: 400/600, Perdida: 0.22136\n",
      "Epoca: 2/5, Paso: 500/600, Perdida: 0.20515\n",
      "Epoca: 2/5, Paso: 600/600, Perdida: 0.08642\n",
      "Epoca: 3/5, Paso: 100/600, Perdida: 0.18626\n",
      "Epoca: 3/5, Paso: 200/600, Perdida: 0.19002\n",
      "Epoca: 3/5, Paso: 300/600, Perdida: 0.22303\n",
      "Epoca: 3/5, Paso: 400/600, Perdida: 0.13499\n",
      "Epoca: 3/5, Paso: 500/600, Perdida: 0.20281\n",
      "Epoca: 3/5, Paso: 600/600, Perdida: 0.26274\n",
      "Epoca: 4/5, Paso: 100/600, Perdida: 0.06373\n",
      "Epoca: 4/5, Paso: 200/600, Perdida: 0.06455\n",
      "Epoca: 4/5, Paso: 300/600, Perdida: 0.11866\n",
      "Epoca: 4/5, Paso: 400/600, Perdida: 0.14202\n",
      "Epoca: 4/5, Paso: 500/600, Perdida: 0.09213\n",
      "Epoca: 4/5, Paso: 600/600, Perdida: 0.10067\n",
      "Epoca: 5/5, Paso: 100/600, Perdida: 0.07346\n",
      "Epoca: 5/5, Paso: 200/600, Perdida: 0.15667\n",
      "Epoca: 5/5, Paso: 300/600, Perdida: 0.05055\n",
      "Epoca: 5/5, Paso: 400/600, Perdida: 0.06640\n",
      "Epoca: 5/5, Paso: 500/600, Perdida: 0.07834\n",
      "Epoca: 5/5, Paso: 600/600, Perdida: 0.16020\n",
      "Precision del modelo en 10000 imagenes: 97.02\n"
     ]
    }
   ],
   "source": [
    "RunModel(\"RN3\", \"Sigm\", \"Adam\", 'runs/RN5_Sigm_Adam', saving_model = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Red Neuronal de 5 capas escondidas + Activaciones ReLU + Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: 1/5, Paso: 100/600, Perdida: 0.47554\n",
      "Epoca: 1/5, Paso: 200/600, Perdida: 0.27499\n",
      "Epoca: 1/5, Paso: 300/600, Perdida: 0.21106\n",
      "Epoca: 1/5, Paso: 400/600, Perdida: 0.23222\n",
      "Epoca: 1/5, Paso: 500/600, Perdida: 0.25960\n",
      "Epoca: 1/5, Paso: 600/600, Perdida: 0.18393\n",
      "Epoca: 2/5, Paso: 100/600, Perdida: 0.12968\n",
      "Epoca: 2/5, Paso: 200/600, Perdida: 0.12914\n",
      "Epoca: 2/5, Paso: 300/600, Perdida: 0.17494\n",
      "Epoca: 2/5, Paso: 400/600, Perdida: 0.23223\n",
      "Epoca: 2/5, Paso: 500/600, Perdida: 0.06750\n",
      "Epoca: 2/5, Paso: 600/600, Perdida: 0.07224\n",
      "Epoca: 3/5, Paso: 100/600, Perdida: 0.05423\n",
      "Epoca: 3/5, Paso: 200/600, Perdida: 0.08579\n",
      "Epoca: 3/5, Paso: 300/600, Perdida: 0.05958\n",
      "Epoca: 3/5, Paso: 400/600, Perdida: 0.22338\n",
      "Epoca: 3/5, Paso: 500/600, Perdida: 0.04557\n",
      "Epoca: 3/5, Paso: 600/600, Perdida: 0.05569\n",
      "Epoca: 4/5, Paso: 100/600, Perdida: 0.14058\n",
      "Epoca: 4/5, Paso: 200/600, Perdida: 0.04078\n",
      "Epoca: 4/5, Paso: 300/600, Perdida: 0.10372\n",
      "Epoca: 4/5, Paso: 400/600, Perdida: 0.10707\n",
      "Epoca: 4/5, Paso: 500/600, Perdida: 0.01342\n",
      "Epoca: 4/5, Paso: 600/600, Perdida: 0.03330\n",
      "Epoca: 5/5, Paso: 100/600, Perdida: 0.13912\n",
      "Epoca: 5/5, Paso: 200/600, Perdida: 0.01556\n",
      "Epoca: 5/5, Paso: 300/600, Perdida: 0.04416\n",
      "Epoca: 5/5, Paso: 400/600, Perdida: 0.03991\n",
      "Epoca: 5/5, Paso: 500/600, Perdida: 0.15979\n",
      "Epoca: 5/5, Paso: 600/600, Perdida: 0.00489\n",
      "Precision del modelo en 10000 imagenes: 97.47\n"
     ]
    }
   ],
   "source": [
    "RunModel(\"RN3\", \"ReLU\", \"Adam\", 'runs/RN5_ReLU_Adam', saving_model = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
